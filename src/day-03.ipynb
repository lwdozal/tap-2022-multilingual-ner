{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Firstname Lastname](https://) for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/).\n",
    "\n",
    "For questions/comments/improvements, email author@email.address.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# `Multilingual NER` `3`\n",
    "\n",
    "This is lesson `3` of 3 in the educational series on `TOPIC`. This notebook is intended `to teach XXX and introduce the concepts of XXXX`. \n",
    "\n",
    "**Audience:** `Teachers` / `Learners` / `Researchers`\n",
    "\n",
    "**Use case:** `Tutorial` / `How-To` / `Reference` / `Explanation` \n",
    "\n",
    "`Include the use case definition from [here](https://constellate.org/docs/documentation-categories)`\n",
    "\n",
    "**Difficulty:** `Beginner` / `Intermediate` / `Advanced`\n",
    "\n",
    "`Beginner assumes users are relatively new to Python and Jupyter Notebooks. The user is helped step-by-step with lots of explanatory text.`\n",
    "`Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.`\n",
    "`Advanced assumes users are very familiar with Python and have been programming for years, but they may not be familiar with the process being explained.`\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "```\n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "* Object-oriented programming (classes, instances, inheritance)\n",
    "* Regular Expressions (`re`, character classes)\n",
    "\n",
    "These should be general skills but can mention a particular library\n",
    "```\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "```\n",
    "* Basic file operations (open, close, read, write)\n",
    "* Data cleaning with `Pandas`\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "```\n",
    "1. Understanding of Machine Learning generally\n",
    "2. Understanding of Transformer Models generally\n",
    "3. Understanding of NER ML\n",
    "4. Understanding of how to do NER ML in spaCy 3\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "`List out any libraries used and what they are used for`\n",
    "* spacy\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (1.22.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: jinja2 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (1.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: setuptools in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wjbmattingly/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "# Using !pip installs\n",
    "!pip install spacy\n",
    "\n",
    "# Using %%bash magic with apt-get and yes prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99377411-f756-4003-994d-6a202c51cf6e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "```\n",
    "In this lesson, we are going to begin learning generally about machine learning which is a branch of artificial intelligence. More specifically, we are going to be discussing deep learning which is itself a field of machine learning.\n",
    "\n",
    "Unlike a traditional computer system in which the human writes the rules to perform a specific task, in machine learning, we use statistics to model a problem. The output is a system where we use statistics to write the rules for us. This will make more sense as we progress through some concrete examples of machine learning.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4753ec-3b8b-4acc-b6bb-2ab9a4ce863c",
   "metadata": {},
   "source": [
    "Before we begin our journey into machine learning, let's start with a fun thinking exercise. I have found it is best to do this as a true story. Take a look at the image below.\n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/68/c8/a0/68c8a0eb4c4ce56e4d54e9df98dfa802.jpg\"\n",
    "     alt=\"Markdown Monster icon\" />\n",
    "     \n",
    "If I were to ask anyone what this is an image of, we would likely all have the same answer. \"Penguin!\" Of course it is. But that's not quite happened when my neighbor's three-year-old child went to my parent's front yard where this very decoration sits every Christmas. To keep this child anonymous, we will simply call him Timmy. Young Timmy did not say \"penguin\" as we all did. Instead, he said \"duck!\"\n",
    "\n",
    "Was Timmy wrong? From our vantage point, yes. He was horribly wrong. Is this his fault? Of course not. He was three. Now, if a grown adult, professor of English came over to the house and made the same comment, we might look at her funny and wonder what she was talking about. We may look behind the penguin to see if there was a duck from the nearby pond lost. Once we realized that she was clearly making a statement about the penguin, we would likely fault her for making an incorrect comment about what that wire-mesh animal truly was.\n",
    "\n",
    "In this scenario, we have a machine learning problem. Young Timmy is a child. He has limited **experiences** in this world. He has a pet duck, a few dogs, and a few cats. He knows what dogs and cats are and he knows what ducks are because of this. In machine learning terms, Timmy's experiences constitute seeing images in his environment and learning what **label** to assign to those things. His parents, like all parents, likely pointed at the dog before he could remember and said, \"dog... dog... dog...\" and likewise with cats and ducks. Let's presume Timmy only knows those three animals. Nothing else.\n",
    "\n",
    "What happened in this scenario? Well, Timmy who was able to classify only three animals out of the millions that exist, met a new out-of-scope animal. Something he had never encountered. He was being asked, in machine learning terms, to generalize on unseen data. Unfortunately for Timmy, that unseen data was unfair. It was something that did not fit into any of the three labels he knew. What did Timmy do in this scenario? He did what any good machine learning model would do. He gave the best answer he could. He said \"duck\".\n",
    "\n",
    "Honestly, Timmy did a great job. He showed the ability to understand the salient features of a duck. **Features** in machine learning terms are the aspects of an item that are important. The pieces of it that make it what it is. In Timmy's case, he likely saw the wings, the bill, and the two feet and came to a quick conclusion. \"Duck!\" These three features clearly make a duck (or a penguin) not a dog or cat. And for these reasons, he gave the label of \"duck!\".\n",
    "\n",
    "What did Timmy's parents do in this scenario? Like all great parents they used the experience to engage in what we call **reinforcement learning** (in machine learning terms). They bent down and pointed at the penguin and said \"no... penguin... penguin... penguin...\". Young Timmy grimaced, looked confused, and moved on with his evening. The nearby cookies had already arrested his attention.\n",
    "\n",
    "Will Timmy call this specific thing a penguin next time he sees it? Possibly. Will he be able to identify a true penguin in the real world? Maybe not. Real penguins look different and Timmy has only had one experience with penguins. He will need many more before he can confidently identify them consistently.\n",
    "\n",
    "At the end of the day, this is how machine learning works. Its the replication of this process in a computer system through statistics and mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7bf26-6a6c-4cfb-9987-04ae0342da31",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41c009-2809-4dc1-8939-dbf3951665df",
   "metadata": {},
   "source": [
    "Supervised learning is the process by which a system learns from a set of inputs that have known labels. In order to train properly, the input data is divided into three categories: training data, validation data, and testing data. There is no set percentage to assign to each of these categories. A good rule of thumb, however, is save 20% of all annotated data for testing and then divide the remaining 80% 80/20 (testing/validation) ratio.\n",
    "\n",
    "The first two, training data and validation data, are given to the system that is trying to learn. It uses the training data to hone a statistical model via predetermined algorithms. It does this by making guesses about what the proper labels are. It then checks its accuracy against the labels provided and makes adjustments accordingly.\n",
    "\n",
    "Once it is finished viewing and guessing across all the training data, the first epoch, or iteration over the data, is finished. At this stage, the model then tests its accuracy against the validation data. These are left out of the training process and give the system a sense of its overall performance.\n",
    "\n",
    "Because the validation data is left out of the training process, it able to be used for mid-training testing (or validation) of its accuracy. The training data is then randomized and given back to the system for x number of epochs. Again, there is no standard for the number of epochs, but a good rule of thumb is to start at 10 and study the results.\n",
    "\n",
    "Once the model repeats this process for the set number of epochs, it is finished training. The model’s accuracy can then be tested against the testing dataset to see how well it performs. The reason you want to keep the testing data separate from the validation data is because, despite being not include in the training, some of the validation data seeps into the training process. Because the testing data is well-annotated, the researcher can get an accurate sense of how well that model performs.\n",
    "\n",
    "With that first model saved, it is common practice to adjust the parameters of the model multiple times to try to create a more accurate model. All models will be tested against the same testing data.\n",
    "\n",
    "At this stage, depending on the results, more training data may need to be obtained, another test may be called for, or the researcher can begin deploying the model on unseen data and examine the results. Unseen data will be data that does not have annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01cac8-e2a1-458d-88dd-6ee4911ec902",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd69aa6-a042-4e4f-980c-97293c621b9a",
   "metadata": {},
   "source": [
    "Word vectors, or word embeddings, take these one dimensional bag of words and gives them multidimensional meaning by representing them in higher dimensional space, noted above. This is achieved through machine learning and can be easily achieved via Python libraries, such as FastText\n",
    "\n",
    "The goal of word vectors is to achieve numerical understanding of language so that a computer can perform more complex tasks on that corpus. Let’s consider the example above. How do we get a computer to understand 2 and 6 are synonyms or mean something similar? One option you might be thinking is to simply give the computer a synonym dictionary. It can look up synonyms and then know what words mean. This approach, on the surface, makes perfect sense, but in practice synonyms are not really the same thing as meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b27a7-af74-43a0-ba4e-afea3edd7c5a",
   "metadata": {},
   "source": [
    "Word vectors have a preset number of dimensions. These dimensions are honed via machine learned. Models take into account word frequency alongside words across a corpus and the appearance of other words in similar contexts. This allows for the the computer to determin the syntactical similarity of words numerically. It then needs to represent these relationships numerically. It does this through the vector, or a matrix of matrices. To represent these more concisely, models flatten a matrix to a float (decimal number). The number of dimensions represent the number of floats in the matrix.\n",
    "\n",
    "Below is a pretrained model’s output of word vectors for Holocaust documents. This is how the word “know” looks in vectors:\n",
    "\n",
    "know -0.19911548 -0.27387282 0.04241912 -0.58703226 0.16149549 -0.08585547 -0.10403373 -0.112367705 -0.28902963 -0.42949626 0.051096343 -0.04708015 -0.051914077 -0.010533272 -0.23334776 0.031974062 -0.015784053 -0.21945408 0.07359381 0.04936823 -0.15373217 -0.18460844 -0.055799782 -0.057939123 0.14816307 -0.46049833 0.16128318 0.190906 -0.29180774 -0.08877125 0.23563664 -0.036557104 -0.23812544 0.21938106 -0.2781296 0.5112853 0.049084224 0.14876273 0.20611146 -0.04535578 -0.35051352 -0.26381743 0.20824358 0.29732847 -0.013382204 -0.19970295 -0.34890386 -0.16214448 -0.23497184 0.1656344 0.15815939 0.012848561 -0.22887675 -0.21618247 0.13367777 0.1028471 0.25068823 -0.13625076 -0.11771541 0.4857257 0.102198474 0.06380113 -0.22328818 -0.05281015 0.0059655504 0.095453635 0.39693353 -0.066147 -0.1920163 0.5153346 0.24972811 -0.0076305643 -0.05530072 -0.24668717 -0.074051596 0.29288396 -0.0849124 0.37786478 0.2398532 -0.10374063 0.5445305 -0.41955113 0.39866814 -0.23992492 -0.15373677 0.34488577 -0.07166888 -0.48001364 0.0660652 0.061260436 0.32197484 -0.12741785 0.024006622 -0.07915035 -0.04467735 -0.2387938 -0.07527494 0.07079664 0.074456714 0.17877163 -0.002122373 -0.16164272 0.12381973 -0.5908519 0.5827627 -0.38076186 0.095964395 0.020342976 -0.5244792 0.24467848 -0.12481717 0.2869162 -0.34473857 -0.19579992 -0.18069582 0.015281798 -0.18330036 -0.08794056 0.015334953 -0.5609912 0.17393902 0.04283724 -0.07696586 0.2040299 0.34686008 0.31219167 0.14669564 -0.26249585 -0.42771882 0.5381632 -0.123247474 -0.29142144 -0.29963812 -0.32800657 -0.10684048 -0.08594837 0.19670585 0.13474767 0.18349588 -0.4734125 0.15554792 -0.21062694 -0.14191462 -0.12800062 0.2053445 -0.05258381 0.10878109 0.56381494 0.22724482 -0.17778987 -0.061046753 0.10789692 -0.015310492 0.16563527 -0.31812978 -0.1478078 0.4323269 -0.2543924 -0.25956103 0.38653126 0.5080214 -0.18796602 -0.10318089 0.023921987 -0.14618908 0.22923793 0.37690258 0.13323267 -0.34325415 -0.048353776 -0.30283198 -0.2839813 -0.2627738 -0.07422618 -0.31940162 0.38072023 0.56700015 -0.023362642 -0.3786432 0.084006436 0.0729958 0.09483505 -0.2665334 0.12699558 -0.37927982 -0.39073908 0.0063185897 -0.34464878 -0.24011964 0.09303968 -0.15488827 -0.018486138 0.3560308 -0.26005003 0.089302294 0.116130605 0.07684872 -0.085253105 -0.28178927 -0.17346472 -0.20008522 0.004347025 0.34192443 0.017453942 0.06926512 -0.15926014 -0.018554512 0.18478563 -0.040194467 0.38450953 0.4104423 -0.016453728 0.013374495 -0.011256633 0.09106963 0.20074937 0.17310189 -0.12467103 0.16330549 -0.0009963055 0.12181527 -0.05295286 -0.0059491103 -0.04697837 0.38616535 -0.21074814 -0.32234505 0.47269863 0.27924335 0.13548143 -0.2677968 0.03536313 0.3248672 0.2062973 0.29093853 0.1844036 -0.43359983 0.025519002 -0.06319317 -0.2427806 -0.22732906 0.08803728 -0.041860744 -0.151291 0.3400458 -0.29143015 0.25334117 0.06265491 0.26399022 -0.20121849 0.22156847 -0.50599706 0.069224015 0.52325517 -0.34115726 -0.105219565 -0.37346402 -0.02126528 0.09619415 0.017722093 -0.3621799 -0.109912336 0.021542747 -0.13361925 0.2087667 -0.08780184 0.09494446 -0.25047818 -0.07924239 0.21750642 0.2621652 -0.52888566 0.081884995 -0.20485449 0.18029206 -0.5623824 -0.03897387 0.3213515 0.057455678 -0.26524526 0.14741589 0.1257589 0.04708992 0.026751317 -0.014696863 -0.11038961 0.004459205 -0.01394376 0.091146186 -0.15486309 0.20662159 -0.0987916 -0.07740813 0.009704136 0.28866896 0.3916269 0.35061485 0.31678385 0.43233085 0.44510433\n",
    "\n",
    "For these vectors, I used the industry-standard of 300 dimensions. We see each of these dimensions represented by each of the floats, separated by whitespace. As the model passes over the corpus it is being trained on, it hones these numbers and changes them for each word. Over multiple epochs, or generations, it gains a clearer sense of the similarity of words, or at least words that are used in similar contexts.\n",
    "\n",
    "Once a word vector model is trained, we can do similarity matches very quickly and very reliably. AI work primarily with Holocaust and human rights abuses documents. For this reason, I will use a word vector model that I have trained on Holocaust documents. Consider the word \"concentration camp\". Let’s now use these word vectors to find the 10 most similar words to concentration camp.\n",
    "\n",
    "Once a word vector model is trained, we can do similarity matches very quickly and very reliably. At the start of the notebook, I asked you to consider the word concentration camp. Let’s now use these word vectors to find the 10 most similar words to concentration camp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b46db39f-5d7a-475c-976b-c774aca70c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('extermination_camp', 0.5768706798553467),\n",
       " ('camp', 0.5369070172309875),\n",
       " ('Flossenbiirg', 0.5099129676818848),\n",
       " ('Sachsenhausen', 0.5068483948707581),\n",
       " ('Auschwitz', 0.48929861187934875),\n",
       " ('Dachau', 0.4765608310699463),\n",
       " ('concen', 0.4753464460372925),\n",
       " ('Majdanek', 0.4740387797355652),\n",
       " ('Sered', 0.47086501121520996),\n",
       " ('Buchenwald', 0.4692303538322449)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    ('extermination_camp', 0.5768706798553467),\n",
    "    ('camp', 0.5369070172309875),\n",
    "    ('Flossenbiirg', 0.5099129676818848),\n",
    "    ('Sachsenhausen', 0.5068483948707581),\n",
    "    ('Auschwitz', 0.48929861187934875),\n",
    "    ('Dachau', 0.4765608310699463),\n",
    "    ('concen', 0.4753464460372925),\n",
    "    ('Majdanek', 0.4740387797355652),\n",
    "    ('Sered', 0.47086501121520996),\n",
    "    ('Buchenwald', 0.4692303538322449)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d7b9f-4d96-4baf-beb7-3f5438779790",
   "metadata": {},
   "source": [
    "These are the items that are most similar to concentration camp in our word vectors. The tuple has two indices. Index 0 is the word and index 1 is the similarity, represented as a float.\n",
    "\n",
    "Exterimination camp is not a direct synonym, as it has a distinction in what happened to prisoners, i.e. execution, however, these are very similar. Seeing this as the most similar word is a sign that the word vectors are well-aligned. Camp is expected as it is a singular word that has similar meaning in context to contentration camp. The remainder of this list are proper nouns, all of which were concentration camps with one exception: “concen”. This is clearly a result of poor cleaning. Concen is not a word, rather a type of concen-tration, most likely. The fact that this is here is also a good sign that our word vectors have aligned well enough to have typos in near vector space.\n",
    "\n",
    "Let’s do something similar with Auschwitz.items that are most similar to concentration camp in our word vectors. The tuple has two indices. Index 0 is the word and index 1 is the similarity, represented as a float.\n",
    "\n",
    "Exterimination camp is not a direct synonym, as it has a distinction in what happened to prisoners, i.e. execution, however, these are very similar. Seeing this as the most similar word is a sign that the word vectors are well-aligned. Camp is expected as it is a singular word that has similar meaning in context to contentration camp. The remainder of this list are proper nouns, all of which were concentration camps with one exception: “concen”. This is clearly a result of poor cleaning. Concen is not a word, rather a type of concen-tration, most likely. The fact that this is here is also a good sign that our word vectors have aligned well enough to have typos in near vector space.\n",
    "\n",
    "Let’s do something similar with Auschwitz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2daa6cd-413a-4d1a-9d5c-07cce252245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Auschwitz_Birkenau', 0.6649479866027832),\n",
       " ('Birkenau', 0.5385118126869202),\n",
       " ('subcamp', 0.5343026518821716),\n",
       " ('camp', 0.533636748790741),\n",
       " ('III', 0.5323576927185059),\n",
       " ('stutthof', 0.518073320388794),\n",
       " ('Ravensbriick', 0.5084848403930664),\n",
       " ('Berlitzer', 0.5083401203155518),\n",
       " ('Malchow', 0.5051567554473877),\n",
       " ('Oswiecim', 0.5016494393348694)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    ('Auschwitz_Birkenau', 0.6649479866027832),\n",
    "    ('Birkenau', 0.5385118126869202),\n",
    "    ('subcamp', 0.5343026518821716),\n",
    "    ('camp', 0.533636748790741),\n",
    "    ('III', 0.5323576927185059),\n",
    "    ('stutthof', 0.518073320388794),\n",
    "    ('Ravensbriick', 0.5084848403930664),\n",
    "    ('Berlitzer', 0.5083401203155518),\n",
    "    ('Malchow', 0.5051567554473877),\n",
    "    ('Oswiecim', 0.5016494393348694)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220ac7f-1210-4f17-bd1d-0d986537ecef",
   "metadata": {},
   "source": [
    "As we can see, the words closest to Auchwitz are places assocaited with Auschwitz, such as Birkenau, subcamps (of which Auschwitz had many), other concentration camps (such as Ravensbriick), and the location of the Auschwitz memorial, Oswiecim.\n",
    "\n",
    "In other words, we have words closely associated with Auschwitz in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68af31-4703-4750-865a-818142566dd5",
   "metadata": {},
   "source": [
    "# Training Sets for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9c450-51f6-4c4a-8b11-79df98eb5920",
   "metadata": {},
   "source": [
    "One of the nice things about spaCy, besides the fact that it scales very well (meaning it can perform well on small data and big data), is that it is easy to customize and perform advanced machine learning methods with little to no knowledge of machine learning. Understanding the basics of ML, however, as discussed in notebook 03 of this series, is helpful because it will allow you to understand how to cultivate a good training set and why certain methods may fail or struggle. In truth, you will develop of a sense of what works and what doesn’t work in ML NER by simply doing it.\n",
    "\n",
    "In Notebook 03 of this series, I mentioned that data for training a machine learning model existed in three forms: training data, validation data, and testing data. All this data will take the same form. It will be a list data structure within which each index will contain a text (a sentence, paragraph, or entire text). The length of this text will depend on what you are hoping to achieve via ML NER. The size of the text will affect the training process. For now, however, let us ignore that. The only other component the training data requires is a list of the entities in that text with their start position, end position, and label. During the training process, these annotations will allow the convolutional neural network (the architecture behind spaCy’s machine learning training process), to learn from the data and be able to correctly identify the entities you are training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f531f5-8786-4b6e-8951-ea03b265d5b4",
   "metadata": {},
   "source": [
    "## What does a spaCy Training Set Look Like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af594fd-822d-4250-97e3-3d3c590466f7",
   "metadata": {},
   "source": [
    "SpaCy requires that your training data be in a very specific form:\n",
    "\n",
    "TRAIN_DATA = [ (TEXT AS A STRING, {“entities”: [(START, END, LABEL)]}) ]\n",
    "\n",
    "Note that TRAIN_DATA is capitalized. It is Pythonic not to capitalize objects with a few exceptions. TRAIN_DATA is one of these exceptions. I don’t know the history of this convention, but in every book/tutorial, you will always see TRAIN_DATA done this way. It is, of course, not necessary, but it is always good practice to be as Pythonic as possible in your code so that others will be able to more easily read your code. Any machine learning practitioner will expect to see TRAIN_DATA as such.\n",
    "\n",
    "Getting the training data into this format is very difficult by hand. A researcher would have to count the characters to assign the start and end of the entity. Even if you consider using Python built-in string functions to get the start and end character, you will run into another problem. The way in which spaCy’s training process reads the start and end characters is different than the way you may count them with the string functions. This means that in the training process, spaCy will drop the annotations that don’t align with the start and end of a token. The reason for this is because of how spaCy tokenizes when compared to how your string functions tokenize the text. Fortunately, there are solutions built into spaCy via the EntityRuler to aid you in this process.\n",
    "\n",
    "If you are interested in manual annotation, I highly encourage you to explore the paid software from Explosion AI, Prodigy (https://prodi.gy/). I am in no way being paid to promote that product. It is expensive, but if you need to do a lot of annotations (for images, text, video, or even audio), then Prodigy is the tool for you. It has a nice user-interface and because it is developed by the same team who gives us spaCy, it can fit seamlessly into a spaCy workflow. You can explore the Prodigy demo here: https://prodi.gy/demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220045c-543d-440d-9a49-5180d6ad7a39",
   "metadata": {},
   "source": [
    "## Creating a Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d228921-b7f7-4bda-94ec-0316f3348474",
   "metadata": {},
   "source": [
    "In the code below, we will make a spaCy machine learning training set via the EntityRuler. In other words, we will use a rules-based method to automatically generate a basic training set. Will this training set have mistakes? Possibly. That’s why it is a good idea to look at the training set and manually verify it. By doing it in this manner, however, you can vastly increase prototyping to see if the custom entity you want to train is potentially viable. In machine learning, there are rarely concrete solutions for domain-specific problems. If there were, people wouldn’t need specialists. Experimentation is often the name of the game in machine learning and it is no different with NER machine learning.\n",
    "\n",
    "We are going to create a blank English model because we will only use this model temporarily. We don’t need the other components. This model with only have an EntityRuler which we will temporarily use to generate the training set. Recall in our last notebook that the spaCy small model could not identify Treblinka correctly as a location? In the below code, we will create a basic training set from these three sentences that will allow us to generate a very small training set. I want to be clear. This training data is nowhere near enough to train a model. This process scales very well, however.\n",
    "\n",
    "Here is the same code we saw before, but with a slightly different text. Note the output. It has correctly identified Treblinka as a GPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6cf3f40-0c21-461a-9e21-a7d5c50ecf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka GPE\n",
      "Treblinka GPE\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "#Sample text\n",
    "text = \"Treblinka is a small village in Poland. Wikipedia notes that Treblinka is not large.\"\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaef338-7cf8-4a79-a64e-70ac73a311e0",
   "metadata": {},
   "source": [
    "Now, we are going to modify this code slightly so that we can generate a slightly different output, one with the start and end of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2296d2e9-23f5-422d-9f95-bfb739674f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka 0 9 GPE\n",
      "Treblinka 61 70 GPE\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Sample text\n",
    "text = \"Treblinka is a small village in Poland. Wikipedia notes that Treblinka is not large.\"\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204d3ab-c2d4-4b15-8ea1-5373814c22f9",
   "metadata": {},
   "source": [
    "Notice now, that our output has 0,9 and 61, 71 for the start and end respectively of each entity. With this data, we can now begin to generate the output we wish. However, let’s try and take the input text and break it down into sentences first to then have two different sets of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "858c62f0-76a3-4c3d-8082-df242fe6df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Treblinka is a small village in Poland.', 'Wikipedia notes that Treblinka is not large.']\n",
      "Treblinka 0 9 GPE\n",
      "Treblinka 21 30 GPE\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "#Sample text\n",
    "text = \"Treblinka is a small village in Poland. Wikipedia notes that Treblinka is not large.\"\n",
    "\n",
    "#Create a blank list for appending later.\n",
    "corpus = []\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#use the spacy tokenizer to get the sentences.\n",
    "for sent in doc.sents:\n",
    "    corpus.append(sent.text)\n",
    "\n",
    "print (corpus)\n",
    "\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "\n",
    "#iterate over the sentences\n",
    "for sentence in corpus:\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    #extract entities\n",
    "    for ent in doc.ents:\n",
    "        print (ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd935c19-0100-480d-870e-697a41022d8f",
   "metadata": {},
   "source": [
    "Notice now we have a different output with different starts and endings. Now, we can once again modify our code to get it into the format we want:\n",
    "\n",
    "TRAIN_DATA = [ (TEXT AS A STRING, {“entities”: [(START, END, LABEL)]}) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "671f48c5-e7de-4c24-bbcc-f2490c5df759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Treblinka is a small village in Poland.', {'entities': [[0, 9, 'GPE']]}]\n",
      "['Wikipedia notes that Treblinka is not large.', {'entities': [[21, 30, 'GPE']]}]\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "#Sample text\n",
    "text = \"Treblinka is a small village in Poland. Wikipedia notes that Treblinka is not large.\"\n",
    "\n",
    "corpus = []\n",
    "\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    corpus.append(sent.text)\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "#iterate over the corpus again\n",
    "for sentence in corpus:\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    #remember, entities needs to be a dictionary in index 1 of the list, so it needs to be an empty list\n",
    "    entities = []\n",
    "    \n",
    "    #extract entities\n",
    "    for ent in doc.ents:\n",
    "\n",
    "        #appending to entities in the correct format\n",
    "        entities.append([ent.start_char, ent.end_char, ent.label_])\n",
    "        \n",
    "    TRAIN_DATA.append([sentence, {\"entities\": entities}])\n",
    "\n",
    "for data in TRAIN_DATA:\n",
    "    print (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3ef66-dfab-458b-a8dc-c1f736029b7b",
   "metadata": {},
   "source": [
    "## How to Convert the Training Data to spaCy Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7d025d5-063b-46a5-9c11-d729c3a415a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from pathlib import Path\n",
    "\n",
    "def convert(lang: str, TRAIN_DATA, output_path: Path):\n",
    "    nlp = spacy.blank(lang)\n",
    "    db = DocBin()\n",
    "    for text, annot in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                msg = f\"Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\\n\\n{repr(text)}\\n\"\n",
    "                warnings.warn(msg)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03193189-a101-4f9c-b035-f00729fd2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(\"en\", TRAIN_DATA, \"../data/train.spacy\")\n",
    "convert(\"en\", TRAIN_DATA, \"../data/valid.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a42ec2-861f-48f5-827c-270aa537909e",
   "metadata": {},
   "source": [
    "## What is the spaCy config.cfg File and How do I create it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea37aa-8ab3-4437-9257-6509ec97df0b",
   "metadata": {},
   "source": [
    "Now that we have our training data ready, it’s time to start preparing our model. In spaCy 3, we have a lot of control over the neural network architecture and hyperperameters of our model. This all takes place in the new config.cfg file. This config file is giving to spaCy during the training process so that it knows what to train and how. In order to create the config.cfg file, we first need to create a base_config.cfg file. To do that, we can use spaCy’s handy GUI, found [here](https://spacy.io/usage/training) (scroll down a bit).\n",
    "\n",
    "For our purposes, select, “English”, the language that we are training, “ner” only, the model we are training, “CPU” (GPU is a bit more complex), and efficiency (quicker to train and smaller because there are no word vectors). You will copy and paste the output in the GUI into your directory as “base_config.cfg”. We will only make two minor changes to this base_config.cfg file. We will specify the path of train and dev (seen under the first category of paths). We will set these to the location of our train.spacy and valid.spacy files.\n",
    "\n",
    "Now that the base_config file is setup correctly, it’s time to convert it to a config.cfg file. To do that, we need to execute a terminal command. Fortunately, we can do that here in Jupyter Notebook. I have placed my base_config file in the subfolder data. By running the command below, spaCy reformats the base_config into a properly formatted config.cfg file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddd32548-2c0b-46fb-9d98-c0a35a9ad214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "../data/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config ../data/base_config.cfg ../data/config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe851c-7faf-41d8-baaa-7802450a705f",
   "metadata": {},
   "source": [
    "## How to Train a spaCy 3 Model from the config.cfg File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7b5f57b-469b-4203-96d5-4f3c58d551ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.cli.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7e8b739-4a07-4ccb-ba04-e8f903b8fc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: ../models/sample\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/sample\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00      7.83   25.00   14.29  100.00    0.25\n",
      "200     200          0.20     98.13  100.00  100.00  100.00    1.00\n",
      "400     400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "600     600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "800     800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1000    1000          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1200    1200          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1400    1400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1600    1600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1800    1800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../models/sample/model-last\n"
     ]
    }
   ],
   "source": [
    "train(\"../data/config.cfg\",\n",
    "      overrides={\"paths.train\": \"../data/train.spacy\",\n",
    "                 \"paths.dev\": \"../data/valid.spacy\"},\n",
    "    output_path=\"../models/sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd730123-2dfc-491b-aff9-cf96e2124fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka GPE\n"
     ]
    }
   ],
   "source": [
    "trained_nlp = spacy.load(\"../models/sample/model-best\")\n",
    "text = \"The village of Treblinka is located in Poland.\"\n",
    "doc = trained_nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad62b82-fd75-4b0b-a11a-7438c5e0d5ae",
   "metadata": {},
   "source": [
    "Note that we gave the machine learning model NER a new sentence and it correctly identifies Treblinka as a “GPE”. But we should not get too excited. Minor alterations to this text result in a missed entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9506c35-9193-4b7d-82d5-a009a1f39eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No entities found.\n"
     ]
    }
   ],
   "source": [
    "text = \"Mark, from New York, said that he wants to go to Treblinkaa to speak to the locals.\"\n",
    "doc = trained_nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)\n",
    "if len(doc.ents) == 0:\n",
    "    print (\"No entities found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ad56f-f687-45cd-9ae8-fceea43e4839",
   "metadata": {},
   "source": [
    "Why does our model now fail? Because we have trained a machine learning model, not an EntityRuler. It knows that Treblinka is a GPE, but it has only learned to identify it if it is spelled correctly. This is a bad model. Machine learing NER models improve with the more training data that we feed them. Most importantly, however, they improve with the greater amount of varied training data we feed them. A good rule of thumb is to start with 200 training samples and then make adjustments going forward. You may need to gather more varied training data or you may need to reconsider your labels. Another possibility is that you need to fine-tune your hyperperameters in the config.cfg file. We will be covering these problems and solutions throughout the remainder of this textbook. By now, though, you should have a good sense of how the training process works in spaCy 3. The material discussed in this notebook are by far the most challenging so far. Take your time here and get to know this process well before moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab5e29-0368-46fd-a65d-42eb239d9e14",
   "metadata": {},
   "source": [
    "# Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7c5cb-798d-4cbd-9974-78032def8730",
   "metadata": {},
   "source": [
    "If your texts have variance in spelling and form with regularity, there are solutions to this problem. For better models in these scenarios, you should consider using transformer models. These models are more robust and are trained to guess the absence of a word in a text. This results in a deeper understanding of the language. Transformer models also learn to recognize sub-word components of a word and store them as sub-word embeddings. This means that transformer models also learn how to recognize out-of-vocabulary (OOV) words or variant spellings that it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83454817-b7e6-4017-93c3-1d7764206342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
