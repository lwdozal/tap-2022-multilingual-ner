{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Firstname Lastname](https://) for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/).\n",
    "\n",
    "For questions/comments/improvements, email author@email.address.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# `Multilingual NER` `2`\n",
    "\n",
    "This is lesson `2` of 3 in the educational series on `multilingual NER`. This notebook is intended `to rules-based NER`. \n",
    "\n",
    "**Audience:** `Teachers` / `Learners` / `Researchers`\n",
    "\n",
    "**Use case:** `Tutorial` / `How-To` / `Reference` / `Explanation` \n",
    "\n",
    "`Include the use case definition from [here](https://constellate.org/docs/documentation-categories)`\n",
    "\n",
    "**Difficulty:** `Beginner` / `Intermediate` / `Advanced`\n",
    "\n",
    "`Beginner assumes users are relatively new to Python and Jupyter Notebooks. The user is helped step-by-step with lots of explanatory text.`\n",
    "`Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.`\n",
    "`Advanced assumes users are very familiar with Python and have been programming for years, but they may not be familiar with the process being explained.`\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "```\n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "* Object-oriented programming (classes, instances, inheritance)\n",
    "```\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "```\n",
    "* Basic file operations (open, close, read, write)\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "```\n",
    "1. Understand How to use spaCy to do NER\n",
    "2. Understand How to Create an EntityRuler\n",
    "3. Understand How to Identify Languages of a Corpus\n",
    "4. Understand A bit about Unsupervised Learning\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "`List out any libraries used and what they are used for`\n",
    "* spaCy - for NLP\n",
    "* spacy_langdetect - for language detection\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2fd0c5a-201c-4247-9182-7c238fcff3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\wma22\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (62.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy_langdetect in c:\\users\\wma22\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: langdetect==1.0.7 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy_langdetect) (1.0.7)\n",
      "Requirement already satisfied: pytest in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy_langdetect) (7.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from langdetect==1.0.7->spacy_langdetect) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pytest->spacy_langdetect) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pytest->spacy_langdetect) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pytest->spacy_langdetect) (21.3)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pytest->spacy_langdetect) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pytest->spacy_langdetect) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pytest->spacy_langdetect) (2.0.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pytest->spacy_langdetect) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pytest->spacy_langdetect) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from packaging->pytest->spacy_langdetect) (3.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bulk in c:\\users\\wma22\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\wma22\\appdata\\roaming\\python\\python38\\site-packages (from bulk) (1.3.5)\n",
      "Requirement already satisfied: typer>=0.4.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from bulk) (0.4.2)\n",
      "Requirement already satisfied: bokeh>=2.4.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from bulk) (2.4.3)\n",
      "Requirement already satisfied: Jinja2>=2.9 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from bokeh>=2.4.3->bulk) (3.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from bokeh>=2.4.3->bulk) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from bokeh>=2.4.3->bulk) (1.21.5)\n",
      "Requirement already satisfied: tornado>=5.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from bokeh>=2.4.3->bulk) (6.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from bokeh>=2.4.3->bulk) (9.0.0)\n",
      "Requirement already satisfied: packaging>=16.8 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from bokeh>=2.4.3->bulk) (21.3)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from bokeh>=2.4.3->bulk) (5.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->bulk) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->bulk) (2019.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from typer>=0.4.1->bulk) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from Jinja2>=2.9->bokeh>=2.4.3->bulk) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from packaging>=16.8->bokeh>=2.4.3->bulk) (3.0.8)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->bulk) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\wma22\\appdata\\roaming\\python\\python38\\site-packages (1.3.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in c:\\users\\wma22\\anaconda3\\lib\\site-packages (0.5.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from umap-learn) (1.8.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from umap-learn) (0.5.7)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from umap-learn) (4.64.0)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from umap-learn) (0.55.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from umap-learn) (1.21.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn) (62.1.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn) (0.38.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from tqdm->umap-learn) (0.4.4)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\wma22\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.6.7)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.21.5)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.18.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.11.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.1.96)\n",
      "Requirement already satisfied: scipy in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.8.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.3.15)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.49)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (7.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (9.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.12)\n",
      "Requirement already satisfied: six in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install spacy_langdetect\n",
    "!pip install bulk\n",
    "!pip install pandas\n",
    "!pip install umap-learn\n",
    "!pip install sentence_transformers\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08514348-bae9-497c-be3f-62e984b4724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the universal sentence encoder\n",
    "model = SentenceTransformer('silencesys/paraphrase-xlm-r-multilingual-v1-fine-tuned-for-latin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa33f3d-8acb-4e7d-9484-adf1b405fff6",
   "metadata": {},
   "source": [
    "# Introduction to spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1f94f-a290-46a4-b3cb-6efb4e7f1f1c",
   "metadata": {},
   "source": [
    "The spaCy (spelled correctly) library is a robust machine learning NLP library developed by Explosion AI, a Berlin based team of computer scientists and computational linguists. It supports a wide variety of European languages out-of-the-box with statistical models capable of parsing texts, identifying parts-of-speech, and extract entities. SpaCy is also capable of easily improving or training from scratch custom models on domain-specific texts.\n",
    "\n",
    "In this notebook, we will go through the steps for installing spaCy, downloading a pretrained language model, and performing the essential tasks of NLP.\n",
    "\n",
    "## Sentence Tokenization\n",
    "\n",
    "A common essential task of NLP is known as tokenization. We looked at tokenization briefly in the last notebook in which we wanted to break a text into individual components. This is one form of tokenization known as word tokenization. There are, however, many other forms, such as sentence tokenization. Sentence tokenization is precisely the same as word tokenization, except instead of breaking a text up into individual word and punctuation components, we break a text up into individual sentences.\n",
    "\n",
    "If you are familiar with Python, you may be familiar with the built-in split() function which allows for a programmer to split a text by whitespace (default) or by passing an argument of a string to define where to split a text, i.e. split(“.”). A common practice (without NLP frameworks) is to split a text into sentences by simply using the split function, but this is ill-advised. Let us consider the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1972efd0-6796-4f57-84da-d1556bb34e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Martin J. Thompson is known for his writing skills. He is also good at programming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2eef0746-500d-4b19-859c-a169f95bf728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Martin J', ' Thompson is known for his writing skills', ' He is also good at programming', '']\n"
     ]
    }
   ],
   "source": [
    "#Now, let's try and use the split function to split the text object based on punctuation.\n",
    "new = text.split(\".\")\n",
    "print (new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff03c2e6-ad40-402b-bd56-1a41ec717e3a",
   "metadata": {},
   "source": [
    "While we successfully were able to split the two sentences, we had the unfortunate result of splitting at Martin J. The reason for this may be obvious. In English, it is common convention to indicate abbreviation with the same punctuation mark used to indicate the end of a sentence. The reason for this extends to the early middle ages when Irish monks began to introduce punctuation and spacing to better read Latin (a story for another day).\n",
    "\n",
    "The very thing that makes texts easier to read, however, greatly hinders our ability to easily split sentences. For this reason, another method is needed. This is where sentence tokenization comes into play. In order to see how sentence tokenization differs, let’s begin with our first spaCy usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28bbc586-a3ec-40e2-a7f9-75ea9968b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we import spaCy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1b5d9-916b-4d23-849f-05521020971c",
   "metadata": {},
   "source": [
    "Next, we need to load an NLP model object. To do this, we use the spacy.load() function. This will take one argument, the model one wishes to load. We will use the small English model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74893c3e-8f54-4ce8-a4a3-a9322cb43219",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361eac93-bbe0-4fab-9ef6-09d8b2199a71",
   "metadata": {},
   "source": [
    "With the nlp object created, we can use it to to parse a text. To do this, we create a doc object. This object will contain a lot of data on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3822520c-2982-41e4-9bf8-73502574ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7dc53ef7-0830-4fde-8da8-803b655c53d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin J. Thompson is known for his writing skills. He is also good at programming.\n"
     ]
    }
   ],
   "source": [
    "print (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83ea56-0358-45e8-927e-daece917effc",
   "metadata": {},
   "source": [
    "While this looks identical to the \"text\" string above, it is quite different. To demonstrate this, let us use the sentence tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92f3b1dd-0d64-488e-b777-a71a5e9f0d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin J. Thompson is known for his writing skills.\n",
      "He is also good at programming.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ead155-bbe0-4679-9860-9120bd32a4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee61835-ba86-4875-b80d-a70c3941da85",
   "metadata": {},
   "source": [
    "# spaCy's Built-In NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbdbc0-64f6-402e-9c8a-05d80476c204",
   "metadata": {},
   "source": [
    "Another essential task of NLP, and the chief subject of this series, is named entity recognition (NER). I spoke about NER in the last notebook. Here, I’d like to demonstrate how to perform basic NER via spaCy. Again, we will iterate over the doc object as we did above, but instead of iterating over doc.sents, we will iterate over doc.ents. For our purposes right now, I simply want to print off each entity’s text (the string itself) and its corresponding label (note the _ after label). I will be explaining this process in much greater detail in the next two notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef33dd7e-e00f-47fb-9df4-2f83b2694111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin J. Thompson PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2f1b1-7fd6-4467-9d55-f026e6b2e47e",
   "metadata": {},
   "source": [
    "As we can see the small spaCy statistical machine learning model has correctly identified that Martin J. Thompson is, in fact, an entity. What kind of entity? A person. We will explore how it made this determination in notebook Day-03 in which we explore machine learning NLP more closely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35545318-8fb3-48ef-bfa1-418f280786b6",
   "metadata": {},
   "source": [
    "# spaCy's EntityRuler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb707b61-895b-47da-b2ec-a94bbf22d28e",
   "metadata": {},
   "source": [
    "The Python library spaCy offers a few different methods for performing rules-based NER. One such method is via its EntityRuler.\n",
    "\n",
    "The EntityRuler is a spaCy factory that allows one to create a set of patterns with corresponding labels. A factory in spaCy is a set of classes and functions preloaded in spaCy that perform set tasks. In the case of the EntityRuler, the factory at hand allows the user to create an EntityRuler, give it a set of instructions, and then use this instructions to find and label entities.\n",
    "\n",
    "Once the user has created the EntityRuler and given it a set of instructions, the user can then add it to the spaCy pipeline as a new pipe. I have spoken in the past notebooks briefly about pipes, but perhaps it is good to address them in more detail here.\n",
    "\n",
    "A pipe is a component of a pipeline. A pipeline’s purpose is to take input data, perform some sort of operations on that input data, and then output those operations either as a new data or extracted metadata. A pipe is an individual component of a pipeline. In the case of spaCy, there are a few different pipes that perform different tasks. The tokenizer, tokenizes the text into individual tokens; the parser, parses the text, and the NER identifies entities and labels them accordingly. All of this data is stored in the Doc object as we saw in Notebook 01_02 of this series.\n",
    "\n",
    "It is important to remember that pipelines are sequential. This means that components earlier in a pipeline affect what later components receive. Sometimes this sequence is essential, meaning later pipes depend on earlier pipes. At other times, this sequence is not essential, meaning later pipes can function without earlier pipes. It is important to keep this in mind as you create custom spaCy models (or any pipeline for that matter).\n",
    "\n",
    "In this notebook, we will be looking closely at the EntityRuler as a component of a spaCy model’s pipeline. Off-the-shelf spaCy models come preloaded with an NER model; they do not, however, come with an EntityRuler. In order to incorperate an EntityRuler into a spaCy model, it must be created as a new pipe, given instructions, and then added to the model. Once this is complete, the user can save that new model with the EntityRuler to the disk.\n",
    "\n",
    "The full documentation of spaCy EntityRuler can be found here: https://spacy.io/api/entityruler .\n",
    "\n",
    "This notebook with synthesize this documentation for non-specialists and provide some examples of it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11b4c182-a823-4390-bc24-9ff0fd308f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poland GPE\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "\n",
    "#Create the Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ed056-ea9a-4203-9796-0c773e21230d",
   "metadata": {},
   "source": [
    "Depending on the version of model you are using, some results may vary.\n",
    "\n",
    "The output from the code above demonstrates spaCy’s small model’s to identify Treblinka, which is a small village in Poland. As the sample text indicates, it was also an extermination camp during WWII. In the first sentence, the spaCy model tagged Treblinka as an LOC (location) and in the second it was missed entirely. Both are either imprecise or wrong. I would have accepted ORG for the second sentence, as spaCy’s model does not know how to classify an extermination camp, but what these results demonstrate is the model’s failure to generalize on data. The reason? There are a few, but I suspect the model never encountered the word Treblinka.\n",
    "\n",
    "This is a common problem in NLP for specific domains. Often times the domains in which we wish to deploy models, off-the-shelf models will fail because they have not been trained on domain-specific texts. We can resolve this, however, either via spaCy’s EntityRuler or via training a new model. As we will see over the next few notebooks, we can use spaCy’s EntityRuler to easily achieve both.\n",
    "\n",
    "For now, let’s first remedy the issue by giving the model instructions for correctly identifying Treblinka. For simplicity, we will use spaCy’s GPE label. In a later notebook, we will teach a model to correctly identify Treblinka in the latter context as a concentration camp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2bfda1af-4171-4991-986b-2b52a1d22df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka GPE\n",
      "Poland GPE\n",
      "Treblinka GPE\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6da87-b756-4932-a403-bded7692b464",
   "metadata": {},
   "source": [
    "If you executed the code above and found that you had the same output, then you did everything correctly. Let's now analyze our pipeline witth nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d93bcffd-88d2-4419-af7e-fc0aa8d2e82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd278cd-abb2-4069-9017-518a2aa90922",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Detecting Languages in Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335be5f-5a9a-4357-aa82-adcec964ed33",
   "metadata": {},
   "source": [
    "Often in multilingual NER, we need to first understand our corpus. In order to do this, we need to analyze all at once to understand what specific languages we need to allow for. If we are working with modern languages, we have several different approaches to do this. First, we can use an off-the-shelf model to identify the languages for a given document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6a840-d056-486b-9ca9-7e907fc6c48f",
   "metadata": {},
   "source": [
    "## Language Detection with SpaCy and LangDetect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31caa5-d142-4e8c-a562-9f292839311f",
   "metadata": {},
   "source": [
    "There are several libraries for doing this in Python, but let's first look at LangDetect which has a wraper for spaCy 2 that we can update to spaCy 3 with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a455e7-ff90-4e71-a566-72fadf751f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://stackoverflow.com/questions/66712753/how-to-use-languagedetector-from-spacy-langdetect-package\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "@Language.factory(\"language_detector\")\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809e6d7-2a32-41a7-b0fb-acff242bc2b4",
   "metadata": {},
   "source": [
    "The above code imports the LanguageDetector class from spacy_langdetect and updates the code in the documentation by correctly assigning it as a factory that we can use. Let's now create a model and load it as a pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4a8131-09b7-4661-90b9-fe0a71ca0cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x21ea046bbb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('language_detector', last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27dde17-211a-474c-93b3-3b11180d5c80",
   "metadata": {},
   "source": [
    "Now that we have the language_detector pipe added to a model, we can use it and access the special attribute, \"language\" that is attached to the Doc container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512a86f7-78b8-4a13-82fc-47afbe2c5030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'en', 'score': 0.9999971086500775}\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an English text.\"\n",
    "doc = nlp(text)\n",
    "print(doc._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d60f623-84ce-4ef4-919f-cec13a88ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'fr', 'score': 0.99999821703433}\n"
     ]
    }
   ],
   "source": [
    "text = \"Ceci est un texte français\"\n",
    "doc = nlp(text)\n",
    "print(doc._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6802d6b-dc5c-4783-b148-d6ea4c904e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'pt', 'score': 0.9999955962555209}\n"
     ]
    }
   ],
   "source": [
    "text = \"Este é um outro texto sem idioma especificado.\"\n",
    "doc = nlp(text)\n",
    "print(doc._.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec96fe-532a-4ba4-987f-4fc4071f7424",
   "metadata": {},
   "source": [
    "## Languages that are not well Represented in Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c20177-d265-4049-8d48-3c748a907947",
   "metadata": {},
   "source": [
    "All of these examples look good, but let's give LangDetect an unfair test, something it never saw before: modern-Irish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db8269a0-b2be-4b90-a6c7-584903890808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'de', 'score': 0.9999951776324398}\n"
     ]
    }
   ],
   "source": [
    "text = \"Seo í an Ghaeilge.\"\n",
    "doc = nlp(text)\n",
    "print(doc._.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14fcdb-98ed-45a4-8b0d-60395683fe14",
   "metadata": {},
   "source": [
    "And here are the limits to something like LangDetect. There are other models available that include less common languages, but one of the things that we can do is we can use machine learning or dictionaries to identify our specific languages that are not represented, e.g. look for the words (or in the case of languages like Old Kannada, which have unique UTF-8 characters, we can even look for the characters) in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519ab65-c5bf-4fc4-8560-4623d6a54b03",
   "metadata": {},
   "source": [
    "## Corpora where there are Multiple Languages inside Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a90f4-3566-4efd-86bb-3e8910017f15",
   "metadata": {},
   "source": [
    "But LangDetect also cannot reliably detect multiple languages. Its classification is a hard one and it has a singular output: one language with one confidence score. What if our text as multiple languages, such as the example below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eed8aa5-d63a-48ec-9adb-28ab3c1b6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_text = '''This is a text where the first line is in English.\n",
    "Maar de tweede regel is in het Nederlands.\n",
    "Dies ist ein deutscher Text.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53c55f-eba2-4590-84cf-4f3ec7328cf5",
   "metadata": {},
   "source": [
    "If we run LangDetect over this text, we get the following output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ff11442-cabf-4304-a3db-7520deb7aca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'en', 'score': 0.42857143962251437}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(large_text)\n",
    "print(doc._.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113331fe-bdc1-4c29-a13b-cbc45c42729c",
   "metadata": {},
   "source": [
    "This is okay. We see that Nederlands (Dutch) is the highest ranking language at 42% confidence. But this text has multiple languages. In this scenario, we need to analyze each sentence. This is common practice when working with multiple languages in a single document in a corpus.\n",
    "\n",
    "To analyze the data correctly, therefore, we need to access the Sentence Container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6bb39473-3835-4e1d-b77a-446fe6a131bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This is a text where the first line is in English.\n",
      "{'language': 'en', 'score': 0.999997222001326}\n",
      "\n",
      "Sentence: Maar de tweede regel is in het Nederlands.\n",
      "{'language': 'nl', 'score': 0.9999955974081092}\n",
      "\n",
      "Sentence: Dies ist ein deutscher Text.\n",
      "{'language': 'de', 'score': 0.9999973271008296}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(f\"Sentence: {sent.text.strip()}\")\n",
    "    print(sent._.language)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325fa0be-2f0c-4472-ba74-ff9c4938adda",
   "metadata": {},
   "source": [
    "## Corpora with Multiple Languages in the Same Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54df95f-a0d3-4189-88e0-b7389f69d94b",
   "metadata": {},
   "source": [
    "Of all the problems, the more challenging problem to solve is dealing with corpora where multiple languages can be found in an individual sentence. In my experience, this usually occurs when non-native speakers of one language need to reference something in their native tongue or when the society that produced the document is strongly bi-lingual that the expectation is speakers and readers would understand both languages equally well. In other instances, I have also seen this occur with quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6593c57d-28ef-47ac-aefc-70d5bd903525",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_sent = \"Josephus vero de historicorum aetate hunc loquitur in modum : Οἱ μέν τοι τὰς ἱστορίας ἐπιχειρήσαντες συγγράφειν παρ᾽ αὐτοῖς, λέγω δὲ τοὺς περὶ Κάδμόν τε τὸν Μιλήσιον, καὶ τὸν Ἀργεῖον Ἀκουσίλαον, καὶ μετὰ τοῦτον εἴ τινες ἄλλοι λέγονται γένεσθαι, βραχὺ τῆς Περσῶν ἐπὶ τὴν ἑλλάδα στρατείας τῷ χρόνῳ προέλαβον: Qui autem historias apud eos conscribere tentavere, id est, Cadmus Milesius et Acusilaus Argivus, et post hunc quicunque alii fuisse feruntur, paulum Persarum expeditionem praecessere .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fdabf32-e9e2-4a96-b08e-be9b6a0c9048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Josephus vero de historicorum aetate hunc loquitur in modum : Οἱ μέν τοι τὰς ἱστορίας ἐπιχειρήσαντες συγγράφειν παρ᾽ αὐτοῖς, λέγω δὲ τοὺς περὶ Κάδμόν τε τὸν Μιλήσιον, καὶ τὸν Ἀργεῖον Ἀκουσίλαον, καὶ μετὰ τοῦτον εἴ τινες ἄλλοι λέγονται γένεσθαι, βραχὺ τῆς Περσῶν ἐπὶ τὴν ἑλλάδα στρατείας τῷ χρόνῳ προέλαβον: Qui autem historias apud eos conscribere tentavere, id est, Cadmus Milesius et Acusilaus Argivus, et post hunc quicunque alii fuisse feruntur, paulum Persarum expeditionem praecessere .\n"
     ]
    }
   ],
   "source": [
    "print(multilingual_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3cd12-ecf0-44ae-bb14-38b9a1685391",
   "metadata": {},
   "source": [
    "Josephus vero de historicorum aetate hunc loquitur in modum : Οἱ μέν τοι τὰς ἱστορίας ἐπιχειρήσαντες συγγράφειν παρ᾽ αὐτοῖς, λέγω δὲ τοὺς περὶ **Κάδμόν** τε τὸν **Μιλήσιον**, καὶ τὸν **Ἀργεῖον Ἀκουσίλαον**, καὶ μετὰ τοῦτον εἴ τινες ἄλλοι λέγονται γένεσθαι, βραχὺ τῆς Περσῶν ἐπὶ τὴν ἑλλάδα στρατείας τῷ χρόνῳ προέλαβον: Qui autem historias apud eos conscribere tentavere, id est, **Cadmus Milesius** et **Acusilaus Argivus**, et post hunc quicunque alii fuisse feruntur, paulum Persarum expeditionem praecessere ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3acfeb65-2b9a-46d1-8435-c088aaf8c832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OMNES latinae linguae Patres, scriptoresque ec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laudandum quidem ingenium, damnanda vero haere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At cum altera pars et melior in fidei certamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hunc merito suum occidentalis Ecclesia sibi vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hujusce proinde magni nominis umbra ac patroci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17391</th>\n",
       "      <td>praeses, duos Tertulliani libros, de Oratione ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17392</th>\n",
       "      <td>Dolendum istud eximium opus non, nisi toto hoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17393</th>\n",
       "      <td>Tantum referre nobis licet, eodem tempore quo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17394</th>\n",
       "      <td>Index analyticus amplissimus tomum tertium abs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395</th>\n",
       "      <td>Send your suggestions, comments or queries to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17396 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      OMNES latinae linguae Patres, scriptoresque ec...\n",
       "1      Laudandum quidem ingenium, damnanda vero haere...\n",
       "2      At cum altera pars et melior in fidei certamin...\n",
       "3      Hunc merito suum occidentalis Ecclesia sibi vi...\n",
       "4      Hujusce proinde magni nominis umbra ac patroci...\n",
       "...                                                  ...\n",
       "17391  praeses, duos Tertulliani libros, de Oratione ...\n",
       "17392  Dolendum istud eximium opus non, nisi toto hoc...\n",
       "17393  Tantum referre nobis licet, eodem tempore quo ...\n",
       "17394  Index analyticus amplissimus tomum tertium abs...\n",
       "17395  Send your suggestions, comments or queries to ...\n",
       "\n",
       "[17396 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/original.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f185384b-121d-47fd-8814-e7a412a91e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Josephus vero de historicorum aetate hunc loquitur in modum : Οἱ μέν τοι τὰς ἱστορίας ἐπιχειρήσαντες συγγράφειν παρ᾽ αὐτοῖς, λέγω δὲ τοὺς περὶ Κάδμόν τε τὸν Μιλήσιον, καὶ τὸν Ἀργεῖον Ἀκουσίλαον, καὶ μετὰ τοῦτον εἴ τινες ἄλλοι λέγονται γένεσθαι, βραχὺ τῆς Περσῶν ἐπὶ τὴν ἑλλάδα στρατείας τῷ χρόνῳ προέλαβον: Qui autem historias apud eos conscribere tentavere, id est, Cadmus Milesius et Acusilaus Argivus, et post hunc quicunque alii fuisse feruntur, paulum Persarum expeditionem praecessere .',\n",
       " 'Nos vero de Graeciae sapientibus et eorum aetate in primo Apparatus nostri tomo disputavimus.',\n",
       " 'De Moysis porro et aliorum prophetarum tempore egimus non solum in eodem citati Apparatus nostri loco, sed in superiori etiam de Lactantii operibus dissertatione.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df.text.tolist()\n",
    "sentences[9872:9875]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0486edaf-0f82-49ab-a12c-f28aa706f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embeddings \n",
    "X =  model.encode(sentences[9800:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3c4cf75-c7ad-4d8f-a57b-8c34c7ea50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensions with UMAP\n",
    "umap = UMAP()\n",
    "X_tfm = umap.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f143a3f1-bf45-4de0-a473-c0169555d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply coordinates\n",
    "new_df = pd.DataFrame(sentences[9800:10000], columns=['text'])\n",
    "new_df['x'] = X_tfm[:, 0]\n",
    "new_df['y'] = X_tfm[:, 1]\n",
    "new_df.to_csv(\"../data/ready_class.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c431b-e89e-4c56-bb8a-cc416c6d0d32",
   "metadata": {},
   "source": [
    "**!!!Do not run in Constellate, please!!! Convert to Code to Use it\n",
    "!python -m bulk text ../data/ready_class.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c658e-5dd7-4ea7-bab4-33ca41058cd4",
   "metadata": {},
   "source": [
    "# EntityRuler with Multilingual Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4b69955-0af0-411e-a719-602ed13f77d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pattern': 'Marius', 'label': 'PERSON'},\n",
       " {'pattern': 'Marii', 'label': 'PERSON'},\n",
       " {'pattern': 'Mario', 'label': 'PERSON'},\n",
       " {'pattern': 'Marium', 'label': 'PERSON'},\n",
       " {'pattern': 'Marie', 'label': 'PERSON'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def first_decliner(root):\n",
    "    endings = [\"us\", \"i\", \"o\", \"um\", \"e\"]\n",
    "    patterns = []\n",
    "    for ending in endings:\n",
    "        patterns.append({\"pattern\": root+ending, \"label\": \"PERSON\"})\n",
    "    return patterns\n",
    "marius = first_decliner(\"Mari\")\n",
    "marius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eca4f526-3ce6-40d0-9cbd-681159c3f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_latin = spacy.blank(\"en\")\n",
    "nlp_latin_ruler = nlp_latin.add_pipe(\"entity_ruler\")\n",
    "nlp_latin_ruler.add_patterns(marius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bd1aec2-13ff-4e82-a282-a6b122e9d00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marius PERSON\n",
      "Marie PERSON\n"
     ]
    }
   ],
   "source": [
    "text = \"Marius was a consul in Rome. Marie is the vocative form.\"\n",
    "doc_latin = nlp_latin(text)\n",
    "for ent in doc_latin.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "633fd4a7-3568-484e-88ed-91735af3aa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marius PERSON\n",
      "Marie PERSON\n"
     ]
    }
   ],
   "source": [
    "def latin_roots(root):\n",
    "    return [{\"pattern\": [{\"TEXT\": {\"REGEX\": \"^\" + root + r\"\\w+\"}}], \"label\": \"PERSON\"}]\n",
    "marius2 = latin_roots(\"Mari\")\n",
    "nlp_latin2 = spacy.blank(\"en\")\n",
    "nlp_latin_ruler2 = nlp_latin2.add_pipe(\"entity_ruler\")\n",
    "nlp_latin_ruler2.add_patterns(marius2)\n",
    "text = \"Marius was a consul in Rome. Marie is the vocative form. Caesar was a dictator.\"\n",
    "doc_latin2 = nlp_latin2(text)\n",
    "for ent in doc_latin2.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb7b9c-e533-42fc-927c-4222a9954394",
   "metadata": {},
   "source": [
    "# Bringing Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "900b31bf-4ba0-4b45-a35d-70503d0d17b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x21ed6de8970>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_detector = spacy.blank('en')\n",
    "lang_detector.add_pipe(\"sentencizer\")\n",
    "lang_detector.add_pipe('language_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d8f175ca-8872-4d5e-a3dc-a77a594df59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_document = \"\"\"This is a story about Margaret who speaks Spanish.\n",
    "'Juan Miguel es mi amigo y tiene veinte años.' Margeret said to her friend Sarah.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "428834e3-5f68-431c-8477-0d1d956ac345",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_nlp = spacy.load(\"en_core_web_sm\")\n",
    "spanish_nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c1be61b2-2d36-4e6e-a985-7b7c5e7cd0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a story about Margaret who speaks Spanish.\n",
      "{'language': 'en', 'score': 0.9999975803853671}\n",
      "Margaret PERSON\n",
      "Spanish LANGUAGE\n",
      "\n",
      "\n",
      "'Juan Miguel es mi amigo y tiene veinte años.'\n",
      "{'language': 'es', 'score': 0.9999951577262021}\n",
      "Juan Miguel es mi amigo y PERSON\n",
      "\n",
      "Margeret said to her friend Sarah.\n",
      "{'language': 'en', 'score': 0.7142832705206366}\n",
      "Sarah PERSON\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in lang_detector(multilingual_document.strip()).sents:\n",
    "    print (sent)\n",
    "    print (sent._.language)\n",
    "    eng_doc = english_nlp(sent.text.strip())\n",
    "    for ent in eng_doc.ents:\n",
    "        print (ent.text, ent.label_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ca96a-9aa2-4ed0-acc5-b32811329759",
   "metadata": {},
   "source": [
    "## Switching between Languages with Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "82a6013a-82ee-48f1-89ea-9c48a453fd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a story about Margaret who speaks Spanish.\n",
      "{'language': 'en', 'score': 0.9999976130185251}\n",
      "Margaret PERSON\n",
      "Spanish LANGUAGE\n",
      "\n",
      "\n",
      "'Juan Miguel es mi amigo y tiene veinte años.'\n",
      "{'language': 'es', 'score': 0.9999979535177211}\n",
      "Juan Miguel PER\n",
      "años. ORG\n",
      "\n",
      "Margeret said to her friend Sarah.\n",
      "{'language': 'en', 'score': 0.7142813289816354}\n",
      "Sarah PERSON\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in lang_detector(multilingual_document.strip()).sents:\n",
    "    print (sent)\n",
    "    print (sent._.language)\n",
    "    if sent._.language[\"language\"] == \"en\":\n",
    "        nested_doc = english_nlp(sent.text.strip())\n",
    "    elif sent._.language['language'] == \"es\":\n",
    "        nested_doc = spanish_nlp(sent.text.strip())\n",
    "    for ent in nested_doc.ents:\n",
    "        print (ent.text, ent.label_)\n",
    "    \n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038da62-74b8-45ec-a932-3e75c1b95926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
