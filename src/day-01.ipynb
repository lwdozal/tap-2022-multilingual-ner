{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Firstname Lastname](https://) for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/).\n",
    "\n",
    "For questions/comments/improvements, email author@email.address.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# `MultiLingual NER`\n",
    "\n",
    "This is lesson `1` of 3 in the educational series on `Named Entity Recognition`. This notebook is intended `the basic problems one faces in multilingual texts`. \n",
    "\n",
    "**Audience:** `Teachers` / `Learners` / `Researchers`\n",
    "\n",
    "**Use case:** `Tutorial` / `How-To` / `Reference` / `Explanation` \n",
    "\n",
    "`Include the use case definition from [here](https://constellate.org/docs/documentation-categories)`\n",
    "\n",
    "**Difficulty:** `Intermediate` / `Advanced`\n",
    "\n",
    "`Beginner assumes users are relatively new to Python and Jupyter Notebooks. The user is helped step-by-step with lots of explanatory text.`\n",
    "`Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.`\n",
    "`Advanced assumes users are very familiar with Python and have been programming for years, but they may not be familiar with the process being explained.`\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "```\n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "* Object-oriented programming (classes, instances, inheritance)\n",
    "* Basic file operations (open, close, read, write)\n",
    "\n",
    "These should be general skills but can mention a particular library\n",
    "```\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "```\n",
    "* Natural Language Processing\n",
    "* spaCy\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "```\n",
    "1. Understand the complexities of multilingual corpora\n",
    "2. Understand text encoding\n",
    "3. Understand how to solve encoding-issues\n",
    "4. Understand how to think about corpora-specific problems\n",
    "5. Understand spaCy\n",
    "6. Understand Named Entity Recognition (NER) as a concept\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "`List out any libraries used and what they are used for`\n",
    "* spaCy\n",
    "* requests\n",
    "* BeautifulSoup (bs4)\n",
    "* unicode\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "# Using !pip installs\n",
    "!pip install spacy\n",
    "!pip install unicode\n",
    "!pip insntall requests\n",
    "!pip install bs4\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517d5c3b-6f43-4b71-a472-c5696ccbad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows the embedding of YouTube videos\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "```\n",
    "In this notebook, we will be covering the major issues and challenges one can face when working with multilingual corpora. We will specifically address how these issues relate to the problem of named entity recognition (NER), a method of information extraction that relies upon natural language processing (NLP).\n",
    "\n",
    "We will also cover the basics of spaCy and named entity recognition in general for those who do not have a background in either area.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a44149-e47f-44b9-a885-b4544a4dabc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a63c21-198c-4858-96e2-478fb41fb350",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9777476-480d-40a9-aeef-89144fc79499",
   "metadata": {},
   "source": [
    "Named entity recognition (addressed below) is a branch of natural language processing, better known as NLP. NLP is the process by which a researcher uses a computer system to parse human language and extract important metadata from texts. The purpose of NLP is to perform, among other things, distant reading.\n",
    "\n",
    "Distant reading has a long history extending to the late-twentieth century. It is commonly used when the quantity of texts in a given corpus prevent a researcher (or a team of researchers) from reading the corpus closely in its entirety. In order to make sense of that large corpus, the researcher will often pass certain tasks to a computer with the understanding that there is a margin of error. This margin of error is accepted in exchange for the ability to gain a larger, distant understanding of that corpus. Distant reading is used to perform several significant tasks, such as:\n",
    "\n",
    "- sentiment analysis=> understanding the sentiment of a text\n",
    "- text classification=> classify texts into predetermined categories\n",
    "- named entity recognition=> extract entities from a text\n",
    "\n",
    "The metadata from these tasks can then be used to get a sense of the texts without reading them closely, hence the term distant reading.\n",
    "\n",
    "The goal of NLP is to feed a text to a computer system and have it return some sort of output. This is often achieved through a series of pipelines that perform some operations on the data at hand.\n",
    "\n",
    "Earlier pipelines, may include a tokenizer, whose sole job is to break a text into individual tokens. Tokens are items in a text that have some linguistic meaning. They can be words, such as “Martha”, but they can also be punctuation marks, such as “,” in the relative clause “, a senior,”. Likewise, “‘nt” in the contraction “can’t” would also be recognized as a token since “‘nt” in English corresponds to the word “not”.\n",
    "\n",
    "A common pipeline after a tokenizer is a POS tagger whose job is to identify the parts-of-speech, or POS, in the text. This is essential for the computer to understand how individual tokens are functioning in a sentence. The way in which we perform POS on different languages is not the same. In inflected languages, such as German, or highly inflected languages, such as Latin or Ancient Greek, the ending of the word contains a lot of information about it’s role in the sentence, i.e. a nominative singular or dative plural. In low inflected languages, such as English, position in the sentence holds primacy. English is a Noun-Verb-Object language (NVO). Let us consider an example sentence:\n",
    "\n",
    "The boy took the ball to the store.\n",
    "\n",
    "The nominative (subject), “boy”, comes first in the sentence, followed by the verb, “took”, then followed by the accusative (object), “ball”, and finally the dative (indirect object), “store”. The words “the” and “to” also contain vital information. “The” occurs twice and tells the reader that it’s not just any ball, it’s the ball; likewise, it’s not just a store, but the store. The period too tells us something important. This is a statement, not a question. For native speakers of a given language these parts-of-speech may go entirely unnoticed. We understand them intuitively. Some of us may have memories of memorizing parsing trees in 5th grade grammar, but for the most part we developed mentally and linguistically with our mother tongue in a unique way. We can use that language without thought of grammar. For those who have devoted time to learning a second language later in life, grammar is a necessity (and sometimes a bane) to learn. We do not learn languages later in life the same way we learn our mother tongue. For a computer, the same holds true. We need to allow the computer to understand parts of speech.\n",
    "\n",
    "Named entity recognition will often times come later in a pipeline because it needs to receive a tokenized text and, in some languages, it needs to understand a words POS to perform well. As a text moves through the pipeline, it receives spans that contain valuable information, such as part of speech. Once the text reaches the NER pipeline, it is time for the machine to make some structured decisions about individual tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8bb44-5732-4a9c-94ad-2b7bd43f1970",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f26a4-8ad5-4974-834e-a15b2f6dbd95",
   "metadata": {},
   "source": [
    "**Entities** are words in a text that correspond to a specific type of data. They can be numerical, such as cardinal numbers; temporal, such as dates; nominal, such as names of people and places; and political, such as geopolitical entities (GPE). In short, an entity can be anything the designer wishes to designate as an item in a text that has a corresponding label.\n",
    "\n",
    "Named entity recognition, or NER, is the process by which a system takes an input of unstructured data (a text) and outputs structured data, specifically the identification of entities. Let us consider this short example.\n",
    "\n",
    "Martha, a senior, moved to Spain where she will be playing basketball until 05 June 2022 or until she can’t play any longer.\n",
    "\n",
    "In this example, we have several potential entities. First, there is “Martha”. Different NER models will have different corresponding labels for such an entity, but PERSON or PER is considered standard practice. Note here that the label is capitalized. This is also standard practice. We also have a GPE, or Geopolitical Entity, notably “Spain”. Finally, we have a DATE entity, “05 June 2022”. These are standard labels that one can expect to extract from a text. If the domain at hand, however, has additional labels, those can be extracted as well. Perhaps the client or user wants to not only extract people, GPEs, and dates, but also sports. In such a scenario “basketball” could be extracted and given the label SPORT.\n",
    "\n",
    "Not all entities are singular. As is common with texts, sometimes entities are **Multi-word Tokens, or MWT**. Let us consider the same sentence as above, but with one modification:\n",
    "\n",
    "`Martha Thompson, a senior, moved to Spain where she will be playing basketball until 05 June 2022 or until she can’t play any longer.`\n",
    "\n",
    "Here, Martha now has a surname, “Thompson”. We can either extract Martha and Thompson as individual entities or, as is more common practice, extract both as a single entity, since “Martha Thompson” is a single individual. An NER system, therefore, should recognize “Martha Thompson” as a single, MWT.\n",
    "\n",
    "As we progress through these notebooks and videos, we will learn new NER concepts. For now, I recommend watching the video below. Each notebook, including this one, will have a corresponding video lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803f59c-d7f8-48ff-af5a-b10aa0b8fbdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766ebd7-066f-460e-b9ab-fae64ba585de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139aafbd-5d1a-452b-bb51-283d6a8e324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for basic web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80658062-55e7-476d-b46d-36e3b7239ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Before we discuss the complexities of text encoding, let's first jump into a real-world example. We will be scraping the data from the SABC SAHA website in South Africa. The page we are scraping, looks like this.\n",
    "\n",
    "<center><img src='../images/saha_website.JPG' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3d18c-07d7-4d31-9032-481c6d5441a0",
   "metadata": {},
   "source": [
    "We are interested in grabbing the data from the p tag in the HTML with id that is equal to line4 (the area I have underlined in the image). This entire page holds a particular testimony from the TRC in South Africa during the late 90s and early 2000s, when this text representation of the testimony was generated. The date and time are important here because this particular text was created with a computer that used an early form of text encoding, or the process by which text is encoded into numerical values that can be parsed by your computer.\n",
    "\n",
    "We know that this has an non-modern standard form of encoding because one character stands out with a �. This character indicates that the text cannot be parsed with the encoding method being used to parse the text. Because we are viewing this on a modern browser (which is based in a modern encoding method known as utf-8), the browser cannot render this particular encoding. Something is happening between the server and our personal computer's browser.\n",
    "\n",
    "Let's grab that particular line using requests and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b1207a94-732a-464a-bdf0-12fbb7757768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "s = requests.get('https://sabctrc.saha.org.za/documents/amntrans/benoni/52831.htm')\n",
    "soup = BeautifulSoup(s.content)\n",
    "line = soup.find(\"p\", {\"id\": \"line4\"})\n",
    "iso_text =  line.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc80e08-eb55-4c90-b331-4f9f552e0464",
   "metadata": {},
   "source": [
    "Now that we have that line, let's print it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4aba6a1e-37d2-4833-b27e-b83cc40fa000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV STEENKAMP: I'm André Steenkamp.\n"
     ]
    }
   ],
   "source": [
    "print (iso_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59339e71-f16f-436c-b014-67177c2a8d26",
   "metadata": {},
   "source": [
    "Everything looks absolutely fine. No issues whatsoever. So, let's save that test to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9c2775cc-2bb0-4539-a1cb-6e1a4b03621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/iso-text.txt', 'w') as f:\n",
    "    f.write(iso_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee39d72-c24f-481c-973a-539a545e04a5",
   "metadata": {},
   "source": [
    "And now, let's open that file up and take another look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2403ddb9-bdaa-4dbe-a9a2-291fba3e53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV STEENKAMP: I'm André Steenkamp.\n"
     ]
    }
   ],
   "source": [
    "with open (\"../data/iso-text.txt\", 'r') as f:\n",
    "    iso_data = f.read()\n",
    "print (iso_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "62607cd8-ca87-4747-86ba-6cb3f1037a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"ADV STEENKAMP: I'm Andr\\xc3\\xa9 Steenkamp.\"\n"
     ]
    }
   ],
   "source": [
    "iso_bytes2 = str.encode(iso_data)\n",
    "print (iso_bytes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e42e6644-b017-4073-974b-ee01cd7a8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/utf8-text.txt\", \"r\") as f:\n",
    "    utf8_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "57a6affc-707d-462e-a447-ed3be83b7815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_data == iso_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9fd78400-18e8-4f69-88b3-55e458ccffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV STEENKAMP: I'm AndrÃ© Steenkamp.\n"
     ]
    }
   ],
   "source": [
    "print(utf8_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3994de08-b61f-4eaa-94e7-301d8bb237e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"ADV STEENKAMP: I'm Andr\\xc3\\x83\\xc2\\xa9 Steenkamp.\"\n"
     ]
    }
   ],
   "source": [
    "utf8_bytes = str.encode(utf8_data)\n",
    "print (utf8_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5ff02497-005b-4e21-a7fb-67e0b3ee49ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"ADV STEENKAMP: I'm Andr\\xc3\\xa9 Steenkamp.\"\n"
     ]
    }
   ],
   "source": [
    "iso_bytes = str.encode(iso_text)\n",
    "print (iso_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28b5ad4b-1e91-4c48-bcc1-5bc158d7de98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iso_bytes == utf8_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21aeda-8e5d-4a04-bf45-d2a8bb63918f",
   "metadata": {},
   "source": [
    "To see what is happening here, let's look at these in a proper text editor. We will use Atom.\n",
    "\n",
    "<center><img src='../images/encoding-issue.JPG' width=500></center>\n",
    "\n",
    "Notice in the above image, we can see the problem is preserved. In the bottom left corner of Atom, however, we have the ability to change the encoding of the text file being observed.\n",
    "\n",
    "<center><img src='../images/encoding-issue2.JPG' width=500></center>\n",
    "\n",
    "When we change that from UTF-8 to ISO-8859-15, the problem vanishes. So, what has happened? We have switched our IDE (Atom) into an encoding method that was used in the 1990s in northern-European languages of which Afrikaans, despite being South African, is associated due to Dutch colonialism.\n",
    "\n",
    "Why is this such a big deal? Because while you or I will see the same text and the same characters on the screen, a computer will not. Those who work with multilingual corpora, especially those who work with texts that were created before the modern day, will encounter at some point corpora that contain multiple encodings. Understanding this issue and the myriad of problems that surface because of them will make your life much easier.\n",
    "\n",
    "This is precisely what we will now address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc26b7-8a1e-4616-a479-52ed45791478",
   "metadata": {},
   "source": [
    "Now, let's return to our variable above, utf8_data. Let's print off utf8_data and see what is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "098ae13c-7332-44fc-b5ba-ad728c17ef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV STEENKAMP: I'm AndrÃ© Steenkamp.\n"
     ]
    }
   ],
   "source": [
    "print(utf8_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c53b2-5c64-4386-b31c-90b5577923df",
   "metadata": {},
   "source": [
    "Let's open up the utf8 file now, with encoding specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0680729c-0289-49c7-a0f0-d587b99750c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/utf8-text.txt\", \"r\") as f:\n",
    "    utf8_data2 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9272a77d-7893-44c0-b027-0199f94a6b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iso_data == utf8_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a1741cd5-1457-4e74-830f-1d22f456cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"ADV STEENKAMP: I'm Andr\\xc3\\x83\\xc2\\xa9 Steenkamp.\"\n",
      "b\"ADV STEENKAMP: I'm Andr\\xc3\\xa9 Steenkamp.\"\n"
     ]
    }
   ],
   "source": [
    "utf8_bytes = str.encode(utf8_data2)\n",
    "print (utf8_bytes)\n",
    "\n",
    "iso_bytes = str.encode(iso_data)\n",
    "print (iso_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda05c7-fffc-4af2-ab90-37c4d4a999f8",
   "metadata": {},
   "source": [
    "So this has now rendered the encoding correctly in Jupyter. Let's see what happens when we open our iso-8859-15 encoded data with utf-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "189a20dd-2e07-44aa-b815-e5f30ecfdee5",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 23: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [140]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/iso-text.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     iso_data \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (iso_data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 23: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "with open (\"../data/iso-text.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    iso_data = f.read()\n",
    "print (iso_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e02b5-b9c2-48a9-85d6-d339ef876ca5",
   "metadata": {},
   "source": [
    "Now we get an error. This error is occuring because Python cannot read the file (which is not encoded in utf-8) as a utf-8-encoded text file. We can use Python, however, to read a different encoding, standardize it into utf-8, and then continue to open that file as a utf-8 file consistently in the future. The process for doing this will vary significantly for other langauges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d08d3-f9ef-47ca-82b0-47f3cde9a948",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baccd12f-0496-4c6a-89d3-69d2f9558621",
   "metadata": {},
   "source": [
    "Tim Scott from Computerphile explains UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c45f61-50e6-43b9-a9bf-f641e4b3d3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wma22\\anaconda3\\lib\\site-packages\\IPython\\core\\display.py:419: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MijmeoH9LT4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MijmeoH9LT4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43faf3e4-8077-49a4-9f20-b834e21d8370",
   "metadata": {},
   "source": [
    "James Briggs Explains Unicode Noramlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a6eb97-2288-4cfc-9691-65be30f2a20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Od9-DV9kd8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Od9-DV9kd8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82649a55-fed8-4d4c-9989-adbb7d52625d",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/what-on-earth-is-unicode-normalization-56c005c55ad0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bbe26-b237-4e04-8d38-46924e346f4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problems within UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d67dc-4b5d-414f-b2e2-356f3baa7f9a",
   "metadata": {},
   "source": [
    "Our problems with encodings, unfortunately, do not end with UTF-8. Once we have encoded our texts into UTF-8, we can still have issues with characters that look the same but being encoded differently. This is particularly true with accented characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b2cd950-aa3d-4386-9d03-473f536c9b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ç\" == \"Ç\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7096345-bb1f-4903-9889-147fef0a6b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ç\" == \"Ç\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5046a64a-deba-415e-a2fc-3ca9733fbce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç\n"
     ]
    }
   ],
   "source": [
    "compound_c = \"\\u0043\\u0327\"\n",
    "print (compound_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad27755e-2210-4d2c-b07d-8a3fe2da95fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç\n"
     ]
    }
   ],
   "source": [
    "accent_c = \"\\u00C7\"\n",
    "print (accent_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d535a-c3be-41cb-9520-777ed2e46088",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalize Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbcf8a37-22a7-47d2-a23a-06ab2f256f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60460cb-e888-4b1b-812e-e024b0572596",
   "metadata": {},
   "source": [
    "| Name | Abbreviation | Description | Example |\n",
    "| --- | --- | --- | --- |\n",
    "| Form D | NFD | *Canonical* decomposition | `Ç` → `C ̧` |\n",
    "| Form C | NFC | *Canoncial* decomposition followed by *canonical* composition | `Ç` → `C ̧` → `Ç` |\n",
    "| Form KD | NFKD | *Compatibility* decomposition | `ℌ ̧` → `H ̧` |\n",
    "| Form KC | NFKC | *Compatibility* decomposition followed by *canonical* composition | `ℌ ̧` → `H ̧` → `Ḩ` |\n",
    "\n",
    "```\n",
    "Source: James Briggs - https://towardsdatascience.com/what-on-earth-is-unicode-normalization-56c005c55ad0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9480309e-641c-4ed3-b392-4239589b689c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_c == accent_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "630b389f-42cb-4855-a2f1-5655d88d0878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç Ç\n"
     ]
    }
   ],
   "source": [
    "print(compound_c, accent_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f901a3f4-1219-4581-aa9a-0fba005aaf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç Ç\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "nfd_compound = unicodedata.normalize('NFD', compound_c)\n",
    "nfd_accent = unicodedata.normalize('NFD', accent_c)\n",
    "print(nfd_compound, nfd_accent)\n",
    "print (nfd_compound == nfd_accent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9448ea24-7271-4443-a28a-ff02c01c001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç Ç\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "nfc_compound = unicodedata.normalize('NFC', compound_c)\n",
    "nfc_accent = unicodedata.normalize('NFC', accent_c)\n",
    "print(nfc_compound, nfc_accent)\n",
    "print (nfc_compound == nfc_accent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87e5184d-23e9-413d-8193-e5d780d38623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfd_example == accent_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6bc505f1-0cbd-461e-9627-a310ddbcda8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV STEENKAMP: I'm AndrÃ© Steenkamp.\n"
     ]
    }
   ],
   "source": [
    "utf8_normalized = unicodedata.normalize('NFC', utf8_data4)\n",
    "print (utf8_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53c780-df1f-4344-b103-075dbb15cfca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Carriage Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66bc89a7-f623-495a-b87b-5344db0d3000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "È\n",
      "È\n"
     ]
    }
   ],
   "source": [
    "e_with_carriage = \"\\u00C8\\u000D\"\n",
    "e_without_carriage = '\\u00C8'\n",
    "print(e_with_carriage)\n",
    "print (e_without_carriage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61c50ad4-ffff-4af5-988d-f0030b9bcc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_with_carriage == e_without_carriage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed6ee88a-eba7-4345-a9d3-8e192df051b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'È'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_without_carriage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20919617-67c3-492b-b9d3-853b4c3f5e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'È\\r'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_with_carriage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97f33a7b-746a-4d99-a052-3d64aaa94612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "È\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('È\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "949f787e-b4fc-47cc-b62d-341bd95b7c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'È\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'È\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90fc89e-8f69-4c14-803b-93543040c408",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Normalize Accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "815e9ec4-0c40-41b1-af4a-d5784ebf95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a10cf27-4dfc-450c-96a5-bdfae30dd99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âcre\n"
     ]
    }
   ],
   "source": [
    "accented_string = \"âcre\"\n",
    "print (accented_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41ea0fb3-aa08-4149-9962-ddce190b2972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acre\n"
     ]
    }
   ],
   "source": [
    "print (unidecode.unidecode(accented_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c91fd-6792-4ab4-b6ae-6334521d8fa0",
   "metadata": {},
   "source": [
    "The problem here is that we have a change in meaning.\n",
    "\n",
    "| French      | English |\n",
    "| ----------- | ----------- |\n",
    "| âcre      | acrid, pungent       |\n",
    "| acre   | acre        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712d4b5-d271-4379-a5ba-3060017764c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
