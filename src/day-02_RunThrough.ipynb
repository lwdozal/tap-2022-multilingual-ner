{"cells":[{"metadata":{},"id":"3e89c5b3","cell_type":"markdown","source":"<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n\nThis notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n\nCreated by [Firstname Lastname](https://) for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/).\n\nFor questions/comments/improvements, email author@email.address.<br />\n____"},{"metadata":{},"id":"68f932d1","cell_type":"markdown","source":"# `Multilingual NER` `2`\n\n### Rules Based Multilingual NER: \nThis is lesson `2` of 3 in the educational series on `multilingual NER`. This notebook is intended `to rules-based NER`. \n\n**Audience:** `Teachers` / `Learners` / `Researchers`\n\n**Use case:** `Tutorial` / `How-To` / `Reference` / `Explanation` \n\n`Include the use case definition from [here](https://constellate.org/docs/documentation-categories)`\n\n**Difficulty:** `Beginner` / `Intermediate` / `Advanced`\n\n`Beginner assumes users are relatively new to Python and Jupyter Notebooks. The user is helped step-by-step with lots of explanatory text.`\n`Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.`\n`Advanced assumes users are very familiar with Python and have been programming for years, but they may not be familiar with the process being explained.`\n\n**Completion time:** `90 minutes`\n\n**Knowledge Required:** \n```\n* Python basics (variables, flow control, functions, lists, dictionaries)\n* Object-oriented programming (classes, instances, inheritance)\n```\n\n**Knowledge Recommended:**\n```\n* Basic file operations (open, close, read, write)\n```\n\n**Learning Objectives:**\nAfter this lesson, learners will be able to:\n```\n1. Understand How to use spaCy to do NER\n2. Understand How to Create an EntityRuler\n3. Understand How to Identify Languages of a Corpus\n4. Understand A bit about Unsupervised Learning\n```\n___"},{"metadata":{},"id":"157c0555","cell_type":"markdown","source":"# Required Python Libraries\n`List out any libraries used and what they are used for`\n* spaCy - for NLP\n* spacy_langdetect - for language detection\n\n## Install Required Libraries"},{"metadata":{"trusted":true},"id":"c2fd0c5a-201c-4247-9182-7c238fcff3b7","cell_type":"code","source":"!pip install spacy\n!pip install spacy_langdetect\n!pip install bulk\n!pip install pandas\n!pip install umap-learn\n!pip install sentence_transformers\n!python -m spacy download en_core_web_sm\n!python -m spacy download es_core_news_sm","execution_count":2,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: spacy in /usr/local/lib/python3.8/site-packages (3.3.1)\nRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/site-packages (from spacy) (0.6.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/site-packages (from spacy) (2.0.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from spacy) (21.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/site-packages (from spacy) (1.8.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from spacy) (2.0.6)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/site-packages (from spacy) (0.9.1)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/site-packages (from spacy) (1.0.7)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from spacy) (3.0.3)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.8/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/site-packages (from spacy) (1.22.4)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/site-packages (from spacy) (2.4.3)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/site-packages (from spacy) (0.7.8)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/site-packages (from spacy) (2.27.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from spacy) (3.0.6)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/site-packages (from spacy) (4.64.0)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/site-packages (from spacy) (1.0.2)\nRequirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.8/site-packages (from spacy) (8.0.17)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.8/site-packages (from spacy) (0.4.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from spacy) (57.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.7)\nRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.2.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: spacy_langdetect in /usr/local/lib/python3.8/site-packages (0.1.2)\nRequirement already satisfied: pytest in /usr/local/lib/python3.8/site-packages (from spacy_langdetect) (7.1.2)\nRequirement already satisfied: langdetect==1.0.7 in /usr/local/lib/python3.8/site-packages (from spacy_langdetect) (1.0.7)\nRequirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from langdetect==1.0.7->spacy_langdetect) (1.16.0)\nRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/site-packages (from pytest->spacy_langdetect) (21.4.0)\nRequirement already satisfied: iniconfig in /usr/local/lib/python3.8/site-packages (from pytest->spacy_langdetect) (1.1.1)\nRequirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.8/site-packages (from pytest->spacy_langdetect) (1.11.0)\nRequirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.8/site-packages (from pytest->spacy_langdetect) (1.0.0)\nRequirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/site-packages (from pytest->spacy_langdetect) (2.0.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.8/site-packages (from pytest->spacy_langdetect) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging->pytest->spacy_langdetect) (3.0.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: bulk in /usr/local/lib/python3.8/site-packages (0.0.4)\nRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.8/site-packages (from bulk) (0.4.1)\nRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.8/site-packages (from bulk) (1.4.3)\nRequirement already satisfied: bokeh>=2.4.3 in /usr/local/lib/python3.8/site-packages (from bulk) (2.4.3)\nRequirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.8/site-packages (from bokeh>=2.4.3->bulk) (6.1)\nRequirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.8/site-packages (from bokeh>=2.4.3->bulk) (21.3)\nRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/site-packages (from bokeh>=2.4.3->bulk) (1.22.4)\nRequirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.8/site-packages (from bokeh>=2.4.3->bulk) (9.1.1)\nRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.8/site-packages (from bokeh>=2.4.3->bulk) (6.0)\nRequirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.8/site-packages (from bokeh>=2.4.3->bulk) (3.0.3)\nRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/site-packages (from bokeh>=2.4.3->bulk) (4.2.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas>=1.0.0->bulk) (2022.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/site-packages (from pandas>=1.0.0->bulk) (2.8.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/site-packages (from typer>=0.4.1->bulk) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from Jinja2>=2.9->bokeh>=2.4.3->bulk) (2.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging>=16.8->bokeh>=2.4.3->bulk) (3.0.7)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.0->bulk) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","name":"stdout"},{"output_type":"stream","text":"\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (1.4.3)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas) (2022.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/site-packages (from pandas) (1.22.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: umap-learn in /usr/local/lib/python3.8/site-packages (0.5.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from umap-learn) (1.22.4)\nRequirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.8/site-packages (from umap-learn) (0.5.7)\nRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/site-packages (from umap-learn) (1.8.1)\nRequirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/site-packages (from umap-learn) (0.56.0)\nRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/site-packages (from umap-learn) (1.1.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from umap-learn) (4.64.0)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (0.39.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (57.5.0)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (4.12.0)\nRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/site-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/site-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.7.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mCollecting sentence_transformers\n  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n  Using cached transformers-4.21.0-py3-none-any.whl (4.7 MB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from sentence_transformers) (4.64.0)\nCollecting torch>=1.6.0\n  Downloading torch-1.12.0-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m937.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision\n  Downloading torchvision-0.13.0-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from sentence_transformers) (1.22.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/site-packages (from sentence_transformers) (1.1.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.8/site-packages (from sentence_transformers) (1.8.1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.8/site-packages (from sentence_transformers) (3.7)\nCollecting sentencepiece\n  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting huggingface-hub>=0.4.0\n  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\nCollecting filelock\n  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.2.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk->sentence_transformers) (1.1.0)\nRequirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk->sentence_transformers) (8.1.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/site-packages (from torchvision->sentence_transformers) (9.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.7)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.8)\nBuilding wheels for collected packages: sentence_transformers\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=016e5da146278d991c2011d4be6f9a792282a06ec5ce834c0f1dd8d87091d131\n  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\nSuccessfully built sentence_transformers\nInstalling collected packages: tokenizers, sentencepiece, torch, filelock, torchvision, huggingface-hub, transformers, sentence_transformers\nSuccessfully installed filelock-3.7.1 huggingface-hub-0.8.1 sentence_transformers-2.2.2 sentencepiece-0.1.96 tokenizers-0.12.1 torch-1.12.0 torchvision-0.13.0 transformers-4.21.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","name":"stdout"},{"output_type":"stream","text":"\u001b[0mCollecting en-core-web-sm==3.3.0\n  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\nRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.8/site-packages (from en-core-web-sm==3.3.0) (3.3.1)\nRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.22.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\nRequirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.8)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.5.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.7)\nRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.2.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\nCollecting es-core-news-sm==3.3.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.3.0/es_core_news_sm-3.3.0-py3-none-any.whl (12.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.8/site-packages (from es-core-news-sm==3.3.0) (3.3.1)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.22.4)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.3.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.6)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.4.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (57.5.0)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.7)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.9.1)\nRequirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.0.17)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.8.2)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.6)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.9)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (21.3)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.27.1)\nRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.6.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.3)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.7.8)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.4.3)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.7)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.64.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.7)\nRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (5.2.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.2.0)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.26.8)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2021.10.8)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.1.1)\nInstalling collected packages: es-core-news-sm\nSuccessfully installed es-core-news-sm-3.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('es_core_news_sm')\n","name":"stdout"}]},{"metadata":{"trusted":true},"id":"5480e2a8","cell_type":"code","source":"### Import Libraries ###\nimport spacy\nimport pandas as pd\nfrom umap import UMAP\nfrom sentence_transformers import SentenceTransformer","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"id":"08514348-bae9-497c-be3f-62e984b4724b","cell_type":"code","source":"# Load the universal sentence encoder\nmodel = SentenceTransformer('silencesys/paraphrase-xlm-r-multilingual-v1-fine-tuned-for-latin')","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a9fb83487b1421fa504adf3d4393682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8167c72e08314f66874826e8c5ef00b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/3.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f48307d5144ebb83b95f54d35da9c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/774 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"412b0e48837b43c18156218f1c454d4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2848aea9ed174459a9a5af4447003ab4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e6c1d1db80e40628854754c00f0abb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1853609c96734cf9a34bcb1b46bf48e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"792abcf25fa04fa598759e8448f9524d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a15012c3eda44d5a5db2a983fefd963"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffbee632f49d4a9ba91102c27eaa4f5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/589 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab2312c7c3514755aa2143a0608086b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c157fe58844d4e6080d1b8cd963d04aa"}},"metadata":{}}]},{"metadata":{},"id":"faa33f3d-8acb-4e7d-9484-adf1b405fff6","cell_type":"markdown","source":"# Introduction to spaCy"},{"metadata":{},"id":"99f1f94f-a290-46a4-b3cb-6efb4e7f1f1c","cell_type":"markdown","source":"The spaCy (spelled correctly) library is a robust machine learning NLP library developed by Explosion AI, a Berlin based team of computer scientists and computational linguists. It supports a wide variety of European languages out-of-the-box with statistical models capable of parsing texts, identifying parts-of-speech, and extract entities. SpaCy is also capable of easily improving or training from scratch custom models on domain-specific texts.\n\nIn this notebook, we will go through the steps for installing spaCy, downloading a pretrained language model, and performing the essential tasks of NLP.\n\n## Sentence Tokenization\n\nA common essential task of NLP is known as tokenization. We looked at tokenization briefly in the last notebook in which we wanted to break a text into individual components. This is one form of tokenization known as word tokenization. There are, however, many other forms, such as sentence tokenization. Sentence tokenization is precisely the same as word tokenization, except instead of breaking a text up into individual word and punctuation components, we break a text up into individual sentences.\n\nIf you are familiar with Python, you may be familiar with the built-in split() function which allows for a programmer to split a text by whitespace (default) or by passing an argument of a string to define where to split a text, i.e. split(“.”). A common practice (without NLP frameworks) is to split a text into sentences by simply using the split function, but this is ill-advised. Let us consider the example below"},{"metadata":{"trusted":true},"id":"1972efd0-6796-4f57-84da-d1556bb34e2c","cell_type":"code","source":"text = \"Martin J. Thompson is known for his writing skills. He is also good at programming.\"","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"id":"2eef0746-500d-4b19-859c-a169f95bf728","cell_type":"code","source":"#Now, let's try and use the split function to split the text object based on punctuation.\nnew = text.split(\".\")\nprint (new)","execution_count":6,"outputs":[{"output_type":"stream","text":"['Martin J', ' Thompson is known for his writing skills', ' He is also good at programming', '']\n","name":"stdout"}]},{"metadata":{},"id":"ff03c2e6-ad40-402b-bd56-1a41ec717e3a","cell_type":"markdown","source":"While we successfully were able to split the two sentences, we had the unfortunate result of splitting at Martin J. The reason for this may be obvious. In English, it is common convention to indicate abbreviation with the same punctuation mark used to indicate the end of a sentence. The reason for this extends to the early middle ages when Irish monks began to introduce punctuation and spacing to better read Latin (a story for another day).\n\nThe very thing that makes texts easier to read, however, greatly hinders our ability to easily split sentences. For this reason, another method is needed. This is where sentence tokenization comes into play. In order to see how sentence tokenization differs, let’s begin with our first spaCy usage."},{"metadata":{"trusted":true},"id":"28bbc586-a3ec-40e2-a7f9-75ea9968b655","cell_type":"code","source":"#First, we import spaCy\nimport spacy","execution_count":7,"outputs":[]},{"metadata":{},"id":"fdd1b5d9-916b-4d23-849f-05521020971c","cell_type":"markdown","source":"Next, we need to load an NLP model object. To do this, we use the spacy.load() function. This will take one argument, the model one wishes to load. We will use the small English model."},{"metadata":{"trusted":true},"id":"74893c3e-8f54-4ce8-a4a3-a9322cb43219","cell_type":"code","source":"#standard spaCy English model\n#more languages at https://spacy.io/usage/models\n\nnlp = spacy.load(\"en_core_web_sm\") #small reach model, there is medium and larger reach models","execution_count":9,"outputs":[]},{"metadata":{},"id":"361eac93-bbe0-4fab-9ef6-09d8b2199a71","cell_type":"markdown","source":"With the nlp object created, we can use it to to parse a text. To do this, we create a doc object. This object will contain a lot of data on the text."},{"metadata":{"trusted":true},"id":"3822520c-2982-41e4-9bf8-73502574ac2f","cell_type":"code","source":"#processed text as an object\ndoc = nlp(text)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"id":"7dc53ef7-0830-4fde-8da8-803b655c53d7","cell_type":"code","source":"print (doc)","execution_count":11,"outputs":[{"output_type":"stream","text":"Martin J. Thompson is known for his writing skills. He is also good at programming.\n","name":"stdout"}]},{"metadata":{},"id":"af83ea56-0358-45e8-927e-daece917effc","cell_type":"markdown","source":"While this looks identical to the \"text\" string above, it is quite different. To demonstrate this, let us use the sentence tokenizer."},{"metadata":{"trusted":true},"id":"92f3b1dd-0d64-488e-b777-a71a5e9f0d27","cell_type":"code","source":"#iterate over sentences\n#tokenize at the sentence level\n# will treat sentences with a semicolon; and handles line breaks -- works with html\n\nfor sent in doc.sents:\n    print (sent)","execution_count":12,"outputs":[{"output_type":"stream","text":"Martin J. Thompson is known for his writing skills.\nHe is also good at programming.\n","name":"stdout"}]},{"metadata":{"trusted":false},"id":"76ead155-bbe0-4679-9860-9120bd32a4bd","cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"id":"9ee61835-ba86-4875-b80d-a70c3941da85","cell_type":"markdown","source":"# spaCy's Built-In NER"},{"metadata":{},"id":"cfbbdbc0-64f6-402e-9c8a-05d80476c204","cell_type":"markdown","source":"Another essential task of NLP, and the chief subject of this series, is named entity recognition (NER). I spoke about NER in the last notebook. Here, I’d like to demonstrate how to perform basic NER via spaCy. Again, we will iterate over the doc object as we did above, but instead of iterating over doc.sents, we will iterate over doc.ents. For our purposes right now, I simply want to print off each entity’s text (the string itself) and its corresponding label (note the _ after label). I will be explaining this process in much greater detail in the next two notebooks."},{"metadata":{"trusted":true},"id":"ef33dd7e-e00f-47fb-9df4-2f83b2694111","cell_type":"code","source":"# ents contains all entity data provided by the pipeline\n\nfor ent in doc.ents:\n    print (ent.text, ent.label_) #underscore provides the label (argmax?)","execution_count":13,"outputs":[{"output_type":"stream","text":"Martin J. Thompson PERSON\n","name":"stdout"}]},{"metadata":{},"id":"ead2f1b1-7fd6-4467-9d55-f026e6b2e47e","cell_type":"markdown","source":"As we can see the small spaCy statistical machine learning model has correctly identified that Martin J. Thompson is, in fact, an entity. What kind of entity? A person. We will explore how it made this determination in notebook Day-03 in which we explore machine learning NLP more closely."},{"metadata":{},"id":"35545318-8fb3-48ef-bfa1-418f280786b6","cell_type":"markdown","source":"# spaCy's EntityRuler"},{"metadata":{},"id":"fb707b61-895b-47da-b2ec-a94bbf22d28e","cell_type":"markdown","source":"The Python library spaCy offers a few different methods for performing rules-based NER. One such method is via its EntityRuler.\n\nThe EntityRuler is a spaCy factory that allows one to create a set of patterns with corresponding labels. A factory in spaCy is a set of classes and functions preloaded in spaCy that perform set tasks. In the case of the EntityRuler, the factory at hand allows the user to create an EntityRuler, give it a set of instructions, and then use this instructions to find and label entities.\n\nOnce the user has created the EntityRuler and given it a set of instructions, the user can then add it to the spaCy pipeline as a new pipe. I have spoken in the past notebooks briefly about pipes, but perhaps it is good to address them in more detail here.\n\nA pipe is a component of a pipeline. A pipeline’s purpose is to take input data, perform some sort of operations on that input data, and then output those operations either as a new data or extracted metadata. A pipe is an individual component of a pipeline. In the case of spaCy, there are a few different pipes that perform different tasks. The tokenizer, tokenizes the text into individual tokens; the parser, parses the text, and the NER identifies entities and labels them accordingly. All of this data is stored in the Doc object as we saw in Notebook 01_02 of this series.\n\nIt is important to remember that pipelines are sequential. This means that components earlier in a pipeline affect what later components receive. Sometimes this sequence is essential, meaning later pipes depend on earlier pipes. At other times, this sequence is not essential, meaning later pipes can function without earlier pipes. It is important to keep this in mind as you create custom spaCy models (or any pipeline for that matter).\n\nIn this notebook, we will be looking closely at the EntityRuler as a component of a spaCy model’s pipeline. Off-the-shelf spaCy models come preloaded with an NER model; they do not, however, come with an EntityRuler. In order to incorperate an EntityRuler into a spaCy model, it must be created as a new pipe, given instructions, and then added to the model. Once this is complete, the user can save that new model with the EntityRuler to the disk.\n\nThe full documentation of spaCy EntityRuler can be found here: https://spacy.io/api/entityruler .\n\nThis notebook with synthesize this documentation for non-specialists and provide some examples of it in action."},{"metadata":{"trusted":true},"id":"11b4c182-a823-4390-bc24-9ff0fd308f30","cell_type":"code","source":"#Import the requisite library\nimport spacy\n\n#Build upon the spaCy Small Model\nnlp = spacy.load(\"en_core_web_sm\")\n\n#Sample text\ntext = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n\n#Create the Doc object\ndoc = nlp(text)\n\n#extract entities\nfor ent in doc.ents:\n    print (ent.text, ent.label_)","execution_count":14,"outputs":[{"output_type":"stream","text":"Poland GPE\n","name":"stdout"}]},{"metadata":{},"id":"031ed056-ea9a-4203-9796-0c773e21230d","cell_type":"markdown","source":"Depending on the version of model you are using, some results may vary.\n\nThe output from the code above demonstrates spaCy’s small model’s to identify Treblinka, which is a small village in Poland. As the sample text indicates, it was also an extermination camp during WWII. In the first sentence, the spaCy model tagged Treblinka as an LOC (location) and in the second it was missed entirely. Both are either imprecise or wrong. I would have accepted ORG for the second sentence, as spaCy’s model does not know how to classify an extermination camp, but what these results demonstrate is the model’s failure to generalize on data. The reason? There are a few, but I suspect the model never encountered the word Treblinka.\n\nThis is a common problem in NLP for specific domains. Often times the domains in which we wish to deploy models, off-the-shelf models will fail because they have not been trained on domain-specific texts. We can resolve this, however, either via spaCy’s EntityRuler or via training a new model. As we will see over the next few notebooks, we can use spaCy’s EntityRuler to easily achieve both.\n\nFor now, let’s first remedy the issue by giving the model instructions for correctly identifying Treblinka. For simplicity, we will use spaCy’s GPE label. In a later notebook, we will teach a model to correctly identify Treblinka in the latter context as a concentration camp."},{"metadata":{"trusted":true},"id":"2bfda1af-4171-4991-986b-2b52a1d22df0","cell_type":"code","source":"#Import the requisite library\nimport spacy\n\n#Build upon the spaCy Small Model\nnlp = spacy.load(\"en_core_web_sm\")\n\n#Sample text\ntext = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n\n#Create the EntityRuler\n#can now act as a variable\nruler = nlp.add_pipe(\"entity_ruler\")\n\n#List of Entities and Patterns\n#SpaCy can assign labels to these patterns\n# list of dictionaries, whose key or label for an entity and the pattern, or keyword\npatterns = [\n                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n            ]\n\nruler.add_patterns(patterns)\n\n\ndoc = nlp(text)\n\n#extract entities\nfor ent in doc.ents:\n    print (ent.text, ent.label_, ent.label)","execution_count":17,"outputs":[{"output_type":"stream","text":"Treblinka GPE 384\nPoland GPE 384\nTreblinka GPE 384\n","name":"stdout"}]},{"metadata":{},"id":"78c6da87-b756-4932-a403-bded7692b464","cell_type":"markdown","source":"If you executed the code above and found that you had the same output, then you did everything correctly. Let's now analyze our pipeline witth nlp.analyze_pipes()"},{"metadata":{"trusted":true},"id":"d93bcffd-88d2-4419-af7e-fc0aa8d2e82b","cell_type":"code","source":"#allows you to look at the structure of your pipeline\n#spam ruler can be very helful for annotation\nnlp.analyze_pipes()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n   'requires': [],\n   'scores': [],\n   'retokenizes': False},\n  'tagger': {'assigns': ['token.tag'],\n   'requires': [],\n   'scores': ['tag_acc'],\n   'retokenizes': False},\n  'parser': {'assigns': ['token.dep',\n    'token.head',\n    'token.is_sent_start',\n    'doc.sents'],\n   'requires': [],\n   'scores': ['dep_uas',\n    'dep_las',\n    'dep_las_per_type',\n    'sents_p',\n    'sents_r',\n    'sents_f'],\n   'retokenizes': False},\n  'attribute_ruler': {'assigns': [],\n   'requires': [],\n   'scores': [],\n   'retokenizes': False},\n  'lemmatizer': {'assigns': ['token.lemma'],\n   'requires': [],\n   'scores': ['lemma_acc'],\n   'retokenizes': False},\n  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n   'requires': [],\n   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n   'retokenizes': False},\n  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n   'requires': [],\n   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n   'retokenizes': False}},\n 'problems': {'tok2vec': [],\n  'tagger': [],\n  'parser': [],\n  'attribute_ruler': [],\n  'lemmatizer': [],\n  'ner': [],\n  'entity_ruler': []},\n 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n  'token.dep': {'assigns': ['parser'], 'requires': []},\n  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n  'doc.sents': {'assigns': ['parser'], 'requires': []},\n  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n  'token.tag': {'assigns': ['tagger'], 'requires': []}}}"},"metadata":{}}]},{"metadata":{"tags":[]},"id":"cbd278cd-abb2-4069-9017-518a2aa90922","cell_type":"markdown","source":"# Detecting Languages in Texts"},{"metadata":{},"id":"0335be5f-5a9a-4357-aa82-adcec964ed33","cell_type":"markdown","source":"Often in multilingual NER, we need to first understand our corpus. In order to do this, we need to analyze all at once to understand what specific languages we need to allow for. If we are working with modern languages, we have several different approaches to do this. First, we can use an off-the-shelf model to identify the languages for a given document."},{"metadata":{},"id":"90a6a840-d056-486b-9ca9-7e907fc6c48f","cell_type":"markdown","source":"## Language Detection with SpaCy and LangDetect"},{"metadata":{},"id":"bb31caa5-d142-4e8c-a562-9f292839311f","cell_type":"markdown","source":"There are several libraries for doing this in Python, but let's first look at LangDetect which has a wraper for spaCy 2 that we can update to spaCy 3 with the code below."},{"metadata":{"trusted":true},"id":"37a455e7-ff90-4e71-a566-72fadf751f54","cell_type":"code","source":"#Source: https://stackoverflow.com/questions/66712753/how-to-use-languagedetector-from-spacy-langdetect-package\nimport spacy\nfrom spacy.language import Language\nfrom spacy_langdetect import LanguageDetector\n\n@Language.factory(\"language_detector\")\ndef get_lang_detector(nlp, name):\n    return LanguageDetector()","execution_count":18,"outputs":[]},{"metadata":{},"id":"b809e6d7-2a32-41a7-b0fb-acff242bc2b4","cell_type":"markdown","source":"The above code imports the LanguageDetector class from spacy_langdetect and updates the code in the documentation by correctly assigning it as a factory that we can use. Let's now create a model and load it as a pipe."},{"metadata":{"trusted":true},"id":"4c4a8131-09b7-4661-90b9-fe0a71ca0cb8","cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe('language_detector', last=True)","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x7f173abc3220>"},"metadata":{}}]},{"metadata":{},"id":"e27dde17-211a-474c-93b3-3b11180d5c80","cell_type":"markdown","source":"Now that we have the language_detector pipe added to a model, we can use it and access the special attribute, \"language\" that is attached to the Doc container."},{"metadata":{"trusted":true},"id":"512a86f7-78b8-4a13-82fc-47afbe2c5030","cell_type":"code","source":"text = \"This is an English text.\"\ndoc = nlp(text)\nprint(doc._.language)","execution_count":20,"outputs":[{"output_type":"stream","text":"{'language': 'en', 'score': 0.9999975649857006}\n","name":"stdout"}]},{"metadata":{"trusted":true},"id":"1d60f623-84ce-4ef4-919f-cec13a88ebef","cell_type":"code","source":"text = \"Ceci est un texte français\"\ndoc = nlp(text)\nprint(doc._.language)","execution_count":21,"outputs":[{"output_type":"stream","text":"{'language': 'fr', 'score': 0.9999954392816763}\n","name":"stdout"}]},{"metadata":{"trusted":true},"id":"b6802d6b-dc5c-4783-b148-d6ea4c904e52","cell_type":"code","source":"text = \"Este é um outro texto sem idioma especificado.\"\ndoc = nlp(text)\nprint(doc._.language)","execution_count":22,"outputs":[{"output_type":"stream","text":"{'language': 'pt', 'score': 0.9999976737392297}\n","name":"stdout"}]},{"metadata":{},"id":"a2ec96fe-532a-4ba4-987f-4fc4071f7424","cell_type":"markdown","source":"## Languages that are not well Represented in Machine Learning Models"},{"metadata":{},"id":"26c20177-d265-4049-8d48-3c748a907947","cell_type":"markdown","source":"All of these examples look good, but let's give LangDetect an unfair test, something it never saw before: modern-Irish."},{"metadata":{"trusted":true},"id":"db8269a0-b2be-4b90-a6c7-584903890808","cell_type":"code","source":"#Gaelic (Modern Irish) Underepresented languages\n\ntext = \"Seo í an Ghaeilge.\"\ndoc = nlp(text)\nprint(doc._.language)","execution_count":23,"outputs":[{"output_type":"stream","text":"{'language': 'de', 'score': 0.7142808371494478}\n","name":"stdout"}]},{"metadata":{},"id":"1d14fcdb-98ed-45a4-8b0d-60395683fe14","cell_type":"markdown","source":"And here are the limits to something like LangDetect. There are other models available that include less common languages, but one of the things that we can do is we can use machine learning or dictionaries to identify our specific languages that are not represented, e.g. look for the words (or in the case of languages like Old Kannada, which have unique UTF-8 characters, we can even look for the characters) in a text."},{"metadata":{},"id":"9519ab65-c5bf-4fc4-8560-4623d6a54b03","cell_type":"markdown","source":"## Corpora where there are Multiple Languages inside Each Document"},{"metadata":{},"id":"d50a90f4-3566-4efd-86bb-3e8910017f15","cell_type":"markdown","source":"But LangDetect also cannot reliably detect multiple languages. Its classification is a hard one and it has a singular output: one language with one confidence score. What if our text as multiple languages, such as the example below?"},{"metadata":{"trusted":true},"id":"2eed8aa5-d63a-48ec-9adb-28ab3c1b6aec","cell_type":"code","source":"large_text = '''This is a text where the first line is in English.\nMaar de tweede regel is in het Nederlands.\nDies ist ein deutscher Text.'''","execution_count":24,"outputs":[]},{"metadata":{},"id":"dc53c55f-eba2-4590-84cf-4f3ec7328cf5","cell_type":"markdown","source":"If we run LangDetect over this text, we get the following output."},{"metadata":{"trusted":true},"id":"4ff11442-cabf-4304-a3db-7520deb7aca6","cell_type":"code","source":"#overall document detection\n\ndoc = nlp(large_text)\nprint(doc._.language)","execution_count":25,"outputs":[{"output_type":"stream","text":"{'language': 'en', 'score': 0.571425575717438}\n","name":"stdout"}]},{"metadata":{},"id":"113331fe-bdc1-4c29-a13b-cbc45c42729c","cell_type":"markdown","source":"This is okay. We see that Nederlands (Dutch) is the highest ranking language at 42% confidence. But this text has multiple languages. In this scenario, we need to analyze each sentence. This is common practice when working with multiple languages in a single document in a corpus.\n\nTo analyze the data correctly, therefore, we need to access the Sentence Container."},{"metadata":{"trusted":true},"id":"6bb39473-3835-4e1d-b77a-446fe6a131bc","cell_type":"code","source":"#each sentence detection\n\nfor sent in doc.sents:\n    print(f\"Sentence: {sent.text.strip()}\")\n    print(sent._.language)\n    print()","execution_count":26,"outputs":[{"output_type":"stream","text":"Sentence: This is a text where the first line is in English.\n{'language': 'en', 'score': 0.9999973440917085}\n\nSentence: Maar de tweede regel is in het Nederlands.\n{'language': 'nl', 'score': 0.9999936320417699}\n\nSentence: Dies ist ein deutscher Text.\n{'language': 'de', 'score': 0.9999975790839888}\n\n","name":"stdout"}]},{"metadata":{},"id":"325fa0be-2f0c-4472-ba74-ff9c4938adda","cell_type":"markdown","source":"## Corpora with Multiple Languages in the Same Sentence"},{"metadata":{},"id":"f54df95f-a0d3-4189-88e0-b7389f69d94b","cell_type":"markdown","source":"Of all the problems, the more challenging problem to solve is dealing with corpora where multiple languages can be found in an individual sentence. In my experience, this usually occurs when non-native speakers of one language need to reference something in their native tongue or when the society that produced the document is strongly bi-lingual that the expectation is speakers and readers would understand both languages equally well. In other instances, I have also seen this occur with quotes."},{"metadata":{"trusted":true},"id":"6593c57d-28ef-47ac-aefc-70d5bd903525","cell_type":"code","source":"#hard shift between two languages\nmultilingual_sent = \"Josephus vero de historicorum aetate hunc loquitur in modum : Οἱ μέν τοι τὰς ἱστορίας ἐπιχειρήσαντες συγγράφειν παρ᾽ αὐτοῖς, λέγω δὲ τοὺς περὶ Κάδμόν τε τὸν Μιλήσιον, καὶ τὸν Ἀργεῖον Ἀκουσίλαον, καὶ μετὰ τοῦτον εἴ τινες ἄλλοι λέγονται γένεσθαι, βραχὺ τῆς Περσῶν ἐπὶ τὴν ἑλλάδα στρατείας τῷ χρόνῳ προέλαβον: Qui autem historias apud eos conscribere tentavere, id est, Cadmus Milesius et Acusilaus Argivus, et post hunc quicunque alii fuisse feruntur, paulum Persarum expeditionem praecessere .\"","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"id":"1fdabf32-e9e2-4a96-b08e-be9b6a0c9048","cell_type":"code","source":"print(multilingual_sent)","execution_count":28,"outputs":[{"output_type":"stream","text":"Josephus vero de historicorum aetate hunc loquitur in modum : Οἱ μέν τοι τὰς ἱστορίας ἐπιχειρήσαντες συγγράφειν παρ᾽ αὐτοῖς, λέγω δὲ τοὺς περὶ Κάδμόν τε τὸν Μιλήσιον, καὶ τὸν Ἀργεῖον Ἀκουσίλαον, καὶ μετὰ τοῦτον εἴ τινες ἄλλοι λέγονται γένεσθαι, βραχὺ τῆς Περσῶν ἐπὶ τὴν ἑλλάδα στρατείας τῷ χρόνῳ προέλαβον: Qui autem historias apud eos conscribere tentavere, id est, Cadmus Milesius et Acusilaus Argivus, et post hunc quicunque alii fuisse feruntur, paulum Persarum expeditionem praecessere .\n","name":"stdout"}]},{"metadata":{},"id":"c0b3cd12-ecf0-44ae-bb14-38b9a1685391","cell_type":"markdown","source":"Josephus vero de historicorum aetate hunc loquitur in modum : Οἱ μέν τοι τὰς ἱστορίας ἐπιχειρήσαντες συγγράφειν παρ᾽ αὐτοῖς, λέγω δὲ τοὺς περὶ **Κάδμόν** τε τὸν **Μιλήσιον**, καὶ τὸν **Ἀργεῖον Ἀκουσίλαον**, καὶ μετὰ τοῦτον εἴ τινες ἄλλοι λέγονται γένεσθαι, βραχὺ τῆς Περσῶν ἐπὶ τὴν ἑλλάδα στρατείας τῷ χρόνῳ προέλαβον: Qui autem historias apud eos conscribere tentavere, id est, **Cadmus Milesius** et **Acusilaus Argivus**, et post hunc quicunque alii fuisse feruntur, paulum Persarum expeditionem praecessere ."},{"metadata":{"trusted":true},"id":"3acfeb65-2b9a-46d1-8435-c088aaf8c832","cell_type":"code","source":"df = pd.read_csv(\"../data/original.csv\")\ndf","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"                                                    text\n0      OMNES latinae linguae Patres, scriptoresque ec...\n1      Laudandum quidem ingenium, damnanda vero haere...\n2      At cum altera pars et melior in fidei certamin...\n3      Hunc merito suum occidentalis Ecclesia sibi vi...\n4      Hujusce proinde magni nominis umbra ac patroci...\n...                                                  ...\n17391  praeses, duos Tertulliani libros, de Oratione ...\n17392  Dolendum istud eximium opus non, nisi toto hoc...\n17393  Tantum referre nobis licet, eodem tempore quo ...\n17394  Index analyticus amplissimus tomum tertium abs...\n17395  Send your suggestions, comments or queries to ...\n\n[17396 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>OMNES latinae linguae Patres, scriptoresque ec...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Laudandum quidem ingenium, damnanda vero haere...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>At cum altera pars et melior in fidei certamin...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hunc merito suum occidentalis Ecclesia sibi vi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hujusce proinde magni nominis umbra ac patroci...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17391</th>\n      <td>praeses, duos Tertulliani libros, de Oratione ...</td>\n    </tr>\n    <tr>\n      <th>17392</th>\n      <td>Dolendum istud eximium opus non, nisi toto hoc...</td>\n    </tr>\n    <tr>\n      <th>17393</th>\n      <td>Tantum referre nobis licet, eodem tempore quo ...</td>\n    </tr>\n    <tr>\n      <th>17394</th>\n      <td>Index analyticus amplissimus tomum tertium abs...</td>\n    </tr>\n    <tr>\n      <th>17395</th>\n      <td>Send your suggestions, comments or queries to ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>17396 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"id":"f185384b-121d-47fd-8814-e7a412a91e3a","cell_type":"code","source":"sentences = df.text.tolist()\nsentences[9872:9875]","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"['Josephus vero de historicorum aetate hunc loquitur in modum : Οἱ μέν τοι τὰς ἱστορίας ἐπιχειρήσαντες συγγράφειν παρ᾽ αὐτοῖς, λέγω δὲ τοὺς περὶ Κάδμόν τε τὸν Μιλήσιον, καὶ τὸν Ἀργεῖον Ἀκουσίλαον, καὶ μετὰ τοῦτον εἴ τινες ἄλλοι λέγονται γένεσθαι, βραχὺ τῆς Περσῶν ἐπὶ τὴν ἑλλάδα στρατείας τῷ χρόνῳ προέλαβον: Qui autem historias apud eos conscribere tentavere, id est, Cadmus Milesius et Acusilaus Argivus, et post hunc quicunque alii fuisse feruntur, paulum Persarum expeditionem praecessere .',\n 'Nos vero de Graeciae sapientibus et eorum aetate in primo Apparatus nostri tomo disputavimus.',\n 'De Moysis porro et aliorum prophetarum tempore egimus non solum in eodem citati Apparatus nostri loco, sed in superiori etiam de Lactantii operibus dissertatione.']"},"metadata":{}}]},{"metadata":{"trusted":true},"id":"0486edaf-0f82-49ab-a12c-f28aa706f4bf","cell_type":"code","source":"# Calculate embeddings \n#takes in individual texts and converting the string into a numerical representation\n#multiple dimensions\n\nX =  model.encode(sentences[9800:10000])","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"id":"e3c4cf75-c7ad-4d8f-a57b-8c34c7ea50ee","cell_type":"code","source":"# Reduce the dimensions with UMAP\n## dimensionality reduction\n# 2 dimensions\n\numap = UMAP()\nX_tfm = umap.fit_transform(X)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"id":"f143a3f1-bf45-4de0-a473-c0169555d143","cell_type":"code","source":"# Apply coordinates\nnew_df = pd.DataFrame(sentences[9800:10000], columns=['text'])\nnew_df['x'] = X_tfm[:, 0]\nnew_df['y'] = X_tfm[:, 1]\nnew_df.to_csv(\"../data/ready_class.csv\", index=False)\n# new_df","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"                                                  text         x         y\n0    Verum his illis verbis, adhuc minus dicimus, e...  5.983134 -2.091753\n1    ARTICULUS V. Quam luculenter Tertullianus prob...  3.770844 -1.901804\n2    Nemo certe diffitebitur librorum Moysis antiqu...  4.913873 -1.405707\n3    Quamobrem Tertullianus veterum scriptorum test...  2.925365 -1.349026\n4    Primum itaque docet Moysem Inacho fuisse coaevum.  3.632606 -2.047698\n..                                                 ...       ...       ...\n195  Verum Tertullianus, qui paucas inter lineas si...  1.960120  0.372385\n196  Alia autem omnia illius mysteria et documenta ...  5.237106 -0.015902\n197  Atque ita quidem Christiani fiunt, non autem n...  4.737965  1.201558\n198   Visne illud tibi ex ipsomet Tertulliano probari?  3.794235 -0.461073\n199  Ad Nationes ille scribit Liberum Patrem cum su...  2.675843 -1.074759\n\n[200 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Verum his illis verbis, adhuc minus dicimus, e...</td>\n      <td>5.983134</td>\n      <td>-2.091753</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ARTICULUS V. Quam luculenter Tertullianus prob...</td>\n      <td>3.770844</td>\n      <td>-1.901804</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Nemo certe diffitebitur librorum Moysis antiqu...</td>\n      <td>4.913873</td>\n      <td>-1.405707</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Quamobrem Tertullianus veterum scriptorum test...</td>\n      <td>2.925365</td>\n      <td>-1.349026</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Primum itaque docet Moysem Inacho fuisse coaevum.</td>\n      <td>3.632606</td>\n      <td>-2.047698</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>Verum Tertullianus, qui paucas inter lineas si...</td>\n      <td>1.960120</td>\n      <td>0.372385</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>Alia autem omnia illius mysteria et documenta ...</td>\n      <td>5.237106</td>\n      <td>-0.015902</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>Atque ita quidem Christiani fiunt, non autem n...</td>\n      <td>4.737965</td>\n      <td>1.201558</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>Visne illud tibi ex ipsomet Tertulliano probari?</td>\n      <td>3.794235</td>\n      <td>-0.461073</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>Ad Nationes ille scribit Liberum Patrem cum su...</td>\n      <td>2.675843</td>\n      <td>-1.074759</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"id":"d83c431b-e89e-4c56-bb8a-cc416c6d0d32","cell_type":"markdown","source":"**!!!Do not run in Constellate, please!!! Convert to Code to Use it\n!python -m bulk text ../data/ready_class.csv**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!python -m bulk text ../data/ready_class.csv**","execution_count":36,"outputs":[{"output_type":"stream","text":"About to serve `bulk` over at http://localhost:5006/.\n^C\n\nAborted!\n","name":"stdout"}]},{"metadata":{},"id":"f97c658e-5dd7-4ea7-bab4-33ca41058cd4","cell_type":"markdown","source":"# EntityRuler with Multilingual Corpora"},{"metadata":{"trusted":true},"id":"b4b69955-0af0-411e-a719-602ed13f77d9","cell_type":"code","source":"#this will decline out a set of patterns\n\ndef first_decliner(root):\n    endings = [\"us\", \"i\", \"o\", \"um\", \"e\"]\n    patterns = []\n    for ending in endings:\n        patterns.append({\"pattern\": root+ending, \"label\": \"PERSON\"})\n    return patterns\nmarius = first_decliner(\"Mari\")\nmarius","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"[{'pattern': 'Marius', 'label': 'PERSON'},\n {'pattern': 'Marii', 'label': 'PERSON'},\n {'pattern': 'Mario', 'label': 'PERSON'},\n {'pattern': 'Marium', 'label': 'PERSON'},\n {'pattern': 'Marie', 'label': 'PERSON'}]"},"metadata":{}}]},{"metadata":{"trusted":true},"id":"eca4f526-3ce6-40d0-9cbd-681159c3f18c","cell_type":"code","source":"nlp_latin = spacy.blank(\"en\")\nnlp_latin_ruler = nlp_latin.add_pipe(\"entity_ruler\")\nnlp_latin_ruler.add_patterns(marius)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"id":"1bd1aec2-13ff-4e82-a282-a6b122e9d00d","cell_type":"code","source":"text = \"Marius was a consul in Rome. Marie is the vocative form.\"\ndoc_latin = nlp_latin(text)\nfor ent in doc_latin.ents:\n    print (ent.text, ent.label_)","execution_count":39,"outputs":[{"output_type":"stream","text":"Marius PERSON\nMarie PERSON\n","name":"stdout"}]},{"metadata":{"trusted":true},"id":"633fd4a7-3568-484e-88ed-91735af3aa21","cell_type":"code","source":"#implement regex; specify that you're looking for a text\n#reference a 1960s way of fuzzy string matching\n\n#always right rules that will be true positives\n\ndef latin_roots(root):\n    return [{\"pattern\": [{\"TEXT\": {\"REGEX\": \"^\" + root + r\"\\w+\"}}], \"label\": \"PERSON\"}]\nmarius2 = latin_roots(\"Mari\")\nnlp_latin2 = spacy.blank(\"en\")\nnlp_latin_ruler2 = nlp_latin2.add_pipe(\"entity_ruler\")\nnlp_latin_ruler2.add_patterns(marius2)\ntext = \"Marius was a consul in Rome. Marie is the vocative form. Caesar was a dictator.\"\ndoc_latin2 = nlp_latin2(text)\nfor ent in doc_latin2.ents:\n    print (ent.text, ent.label_)","execution_count":40,"outputs":[{"output_type":"stream","text":"Marius PERSON\nMarie PERSON\n","name":"stdout"}]},{"metadata":{},"id":"28cb7b9c-e533-42fc-927c-4222a9954394","cell_type":"markdown","source":"# Bringing Everything Together"},{"metadata":{"trusted":true},"id":"900b31bf-4ba0-4b45-a35d-70503d0d17b3","cell_type":"code","source":"lang_detector = spacy.blank('en')\nlang_detector.add_pipe(\"sentencizer\")\nlang_detector.add_pipe('language_detector')","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x7f1720bf37c0>"},"metadata":{}}]},{"metadata":{"trusted":true},"id":"d8f175ca-8872-4d5e-a3dc-a77a594df59f","cell_type":"code","source":"multilingual_document = \"\"\"This is a story about Margaret who speaks Spanish.\n'Juan Miguel es mi amigo y tiene veinte años.' Margeret said to her friend Sarah.\n\"\"\"","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"id":"428834e3-5f68-431c-8477-0d1d956ac345","cell_type":"code","source":"english_nlp = spacy.load(\"en_core_web_sm\")\nspanish_nlp = spacy.load(\"es_core_news_sm\")","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"id":"c1be61b2-2d36-4e6e-a985-7b7c5e7cd0e1","cell_type":"code","source":"for sent in lang_detector(multilingual_document.strip()).sents:\n    print (sent)\n    print (sent._.language)\n    eng_doc = english_nlp(sent.text.strip())\n    for ent in eng_doc.ents:\n        print (ent.text, ent.label_)\n    print()","execution_count":44,"outputs":[{"output_type":"stream","text":"This is a story about Margaret who speaks Spanish.\n{'language': 'en', 'score': 0.999998003935004}\nMargaret PERSON\nSpanish LANGUAGE\n\n\n'Juan Miguel es mi amigo y tiene veinte años.'\n{'language': 'es', 'score': 0.999996897054559}\nJuan Miguel es mi amigo y PERSON\n\nMargeret said to her friend Sarah.\n{'language': 'en', 'score': 0.5919624592921453}\nSarah PERSON\n\n","name":"stdout"}]},{"metadata":{},"id":"a35ca96a-9aa2-4ed0-acc5-b32811329759","cell_type":"markdown","source":"## Switching between Languages with Conditionals"},{"metadata":{"trusted":true},"id":"82a6013a-82ee-48f1-89ea-9c48a453fd05","cell_type":"code","source":"for sent in lang_detector(multilingual_document.strip()).sents:\n    print (sent)\n    print (sent._.language)\n    if sent._.language[\"language\"] == \"en\":\n        nested_doc = english_nlp(sent.text.strip())\n    elif sent._.language['language'] == \"es\":\n        nested_doc = spanish_nlp(sent.text.strip())\n    for ent in nested_doc.ents:\n        print (ent.text, ent.label_)\n    \n    print ()","execution_count":45,"outputs":[{"output_type":"stream","text":"This is a story about Margaret who speaks Spanish.\n{'language': 'en', 'score': 0.9999950105339779}\nMargaret PERSON\nSpanish LANGUAGE\n\n\n'Juan Miguel es mi amigo y tiene veinte años.'\n{'language': 'es', 'score': 0.9999968036610237}\nJuan Miguel PER\naños. ORG\n\nMargeret said to her friend Sarah.\n{'language': 'en', 'score': 0.714278268862856}\nSarah PERSON\n\n","name":"stdout"}]},{"metadata":{"trusted":false},"id":"3038da62-74b8-45ec-a932-3e75c1b95926","cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"base_numbering":1,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":5}